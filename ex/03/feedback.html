<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>31d0826ce5834f17a0297c1d46b64b55</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="task-1" class="cell markdown" id="qulp2CeTu6Bp">
<h3>Task 1:</h3>
</section>
<div class="cell code" data-execution_count="2" id="3oF1N8Ewuuko">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.datasets <span class="im">as</span> datasets</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> conv2d, max_pool2d, cross_entropy</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3" id="Qya5qkPwuukq">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.rc(<span class="st">&quot;figure&quot;</span>, dpi<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># transform images into normalized tensors</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>(<span class="fl">0.5</span>,), std<span class="op">=</span>(<span class="fl">0.5</span>,))</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> datasets.MNIST(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;./&quot;</span>,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> datasets.MNIST(</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;./&quot;</span>,</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>train_dataset,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    dataset<span class="op">=</span>test_dataset,</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    pin_memory<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weights(shape):</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Kaiming He initialization (a good initialization is important)</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://arxiv.org/abs/1502.01852</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">#use GPU acceleration</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span>  torch.device(<span class="st">&#39;cuda:0&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> np.sqrt(<span class="fl">2.</span> <span class="op">/</span> shape[<span class="dv">0</span>])</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> torch.randn(size<span class="op">=</span>shape, device <span class="op">=</span> device) <span class="op">*</span> std</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    w.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rectify(x):</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rectified Linear Unit (ReLU)</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.<span class="bu">max</span>(torch.zeros_like(x), x)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RMSprop(optim.Optimizer):</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="co">    This is a reduced version of the PyTorch internal RMSprop optimizer</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="co">    It serves here as an example</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr<span class="op">=</span><span class="fl">1e-3</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        defaults <span class="op">=</span> <span class="bu">dict</span>(lr<span class="op">=</span>lr, alpha<span class="op">=</span>alpha, eps<span class="op">=</span>eps)</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(RMSprop, <span class="va">self</span>).<span class="fu">__init__</span>(params, defaults)</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> group <span class="kw">in</span> <span class="va">self</span>.param_groups:</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p <span class="kw">in</span> group[<span class="st">&#39;params&#39;</span>]:</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>                grad <span class="op">=</span> p.grad.data</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>                state <span class="op">=</span> <span class="va">self</span>.state[p]</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>                <span class="co"># state initialization</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(state) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>                    state[<span class="st">&#39;square_avg&#39;</span>] <span class="op">=</span> torch.zeros_like(p.data)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>                square_avg <span class="op">=</span> state[<span class="st">&#39;square_avg&#39;</span>]</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>                alpha <span class="op">=</span> group[<span class="st">&#39;alpha&#39;</span>]</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>                <span class="co"># update running averages</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>                square_avg.mul_(alpha).addcmul_(grad, grad, value<span class="op">=</span><span class="dv">1</span> <span class="op">-</span> alpha)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>                avg <span class="op">=</span> square_avg.sqrt().add_(group[<span class="st">&#39;eps&#39;</span>])</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>                <span class="co"># gradient update</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>                p.data.addcdiv_(grad, avg, value<span class="op">=-</span>group[<span class="st">&#39;lr&#39;</span>])</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="4" id="24K1zB4Suuku">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_model(model, params ,n_epochs <span class="op">=</span> <span class="dv">100</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">len</span>(params))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> []</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> RMSprop(params<span class="op">=</span>params)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#define device</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">&#39;cuda:0&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;Device:&#39;</span>, device)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    torch.cuda.set_device(device)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># put this into a training loop over 100 epochs</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        train_loss_this_epoch <span class="op">=</span> []</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            x, y <span class="op">=</span> batch</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># our model requires flattened input</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x.reshape(batch_size, <span class="dv">784</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># feed input through model</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            <span class="co">#noise_py_x = model(x, w_h, w_h2, w_o)</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            noise_py_x <span class="op">=</span> model(x, <span class="op">*</span>params)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># reset the gradient</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            <span class="co"># the cross-entropy loss function already contains the softmax</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> cross_entropy(noise_py_x, y, reduction<span class="op">=</span><span class="st">&quot;mean&quot;</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>            train_loss_this_epoch.append(<span class="bu">float</span>(loss.to(<span class="st">&quot;cpu&quot;</span>)))</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># compute the gradient</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>            <span class="co"># update weights</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        train_loss.append(np.mean(train_loss_this_epoch))</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># test periodically</span></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Epoch: </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Mean Train Loss: </span><span class="sc">{</span>train_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2e}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>            test_loss_this_epoch <span class="op">=</span> []</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># no need to compute gradients for validation</span></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> idx, batch <span class="kw">in</span> <span class="bu">enumerate</span>(test_dataloader):</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>                    x, y <span class="op">=</span> batch</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>                    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>                    x <span class="op">=</span> x.reshape(batch_size, <span class="dv">784</span>)</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>                    noise_py_x <span class="op">=</span> model(x, <span class="op">*</span>params)</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>                    loss <span class="op">=</span> cross_entropy(noise_py_x, y, reduction<span class="op">=</span><span class="st">&quot;mean&quot;</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>                    test_loss_this_epoch.append(<span class="bu">float</span>(loss.cpu()))</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>            test_loss.append(np.mean(test_loss_this_epoch))</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Mean Test Loss:  </span><span class="sc">{</span>test_loss[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2e}</span><span class="ss">&quot;</span>)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_loss, test_loss</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_results(train_loss, test_loss, n_epochs, label_addition <span class="op">=</span> <span class="st">&quot;&quot;</span>):</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(n_epochs <span class="op">+</span> <span class="dv">1</span>), train_loss, label<span class="op">=</span><span class="ss">f&quot;</span><span class="sc">{</span>label_addition<span class="sc">}</span><span class="ss">_Train&quot;</span>)</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(<span class="dv">1</span>, n_epochs <span class="op">+</span> <span class="dv">2</span>, <span class="dv">10</span>), test_loss, label<span class="op">=</span><span class="ss">f&quot;</span><span class="sc">{</span>label_addition<span class="sc">}</span><span class="ss">_Test&quot;</span>)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Train and Test Loss over Training&quot;</span>)</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Epoch&quot;</span>)</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Loss&quot;</span>)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span></code></pre></div>
</div>
<div class="cell markdown">

</div>
<section id="model-from-intro" class="cell markdown" id="JPuxz_bfuukv">
<h2>model from intro</h2>
</section>
<div class="cell code" data-execution_count="22"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="GEigrsvsuukx" data-outputId="6f145ed5-c432-4ada-8802-b9d5f61f07e7">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#epochs to train</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define the neural network</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> intro_model(x, w_h, w_h2, w_o):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> rectify(x <span class="op">@</span> w_h)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    h2 <span class="op">=</span> rectify(h <span class="op">@</span> w_h2)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    pre_softmax <span class="op">=</span> h2 <span class="op">@</span> w_o</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pre_softmax</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># input shape is (B, 784)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>w_h <span class="op">=</span> init_weights((<span class="dv">784</span>, <span class="dv">625</span>))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer with 625 neurons</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>w_h2 <span class="op">=</span> init_weights((<span class="dv">625</span>, <span class="dv">625</span>))</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer with 625 neurons</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>w_o <span class="op">=</span> init_weights((<span class="dv">625</span>, <span class="dv">10</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># output shape is (B, 10)</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>intro_params <span class="op">=</span> [w_h, w_h2, w_o]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>intro_train_loss, intro_test_loss <span class="op">=</span> run_model(intro_model, intro_params, n_epochs)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plot_results(intro_train_loss,intro_test_loss, n_epochs, <span class="st">&quot;intro&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>3
Device: cuda:0
Epoch: 0
Mean Train Loss: 3.95e-01
Mean Test Loss:  1.69e-01
Epoch: 10
Mean Train Loss: 1.57e-01
Mean Test Loss:  2.36e-01
Epoch: 20
Mean Train Loss: 9.68e-02
Mean Test Loss:  3.37e-01
Epoch: 30
Mean Train Loss: 6.37e-02
Mean Test Loss:  4.73e-01
Epoch: 40
Mean Train Loss: 5.50e-02
Mean Test Loss:  6.74e-01
Epoch: 50
Mean Train Loss: 3.37e-02
Mean Test Loss:  7.58e-01
Epoch: 60
Mean Train Loss: 2.76e-02
Mean Test Loss:  8.20e-01
Epoch: 70
Mean Train Loss: 2.71e-02
Mean Test Loss:  6.54e-01
Epoch: 80
Mean Train Loss: 1.61e-02
Mean Test Loss:  9.31e-01
Epoch: 90
Mean Train Loss: 9.79e-03
Mean Test Loss:  8.42e-01
Epoch: 100
Mean Train Loss: 8.77e-03
Mean Test Loss:  1.04e+00
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/f4ec019a3c09c73061e76f45dfa955ea8c6bf338.png" /></p>
</div>
</div>
<div class="cell markdown">
<div style="color: green; font-weight: bold">Comment</div>
<p>Seems correct</p>
</div>
<section id="task-2-drop-out-layer" class="cell markdown"
id="JqvT8SSX6MtY">
<h3>Task 2: Drop-out layer</h3>
</section>
<div class="cell code" data-execution_count="23" id="DZb6mLYE6Lr3">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#define dropout layer</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dropout(X , p_drop <span class="op">=</span> <span class="fl">0.5</span>) :</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#check device in case of GPU </span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">&#39;cuda:0&#39;</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#return unchanged X</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> p_drop <span class="op">&lt;=</span> <span class="fl">0.0</span> <span class="kw">or</span> p_drop <span class="op">&gt;=</span> <span class="fl">1.0</span>:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#create mask from dropout threshold using numbers generated by binomial distribution</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> torch.bernoulli(torch.ones(X.size()) <span class="op">*</span> p_drop).<span class="bu">type</span>(torch.<span class="bu">bool</span>).to(device)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">#now set 0 if mask value is 1 else set it to xi/(1-p_drop)</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.where(mask, <span class="dv">0</span>, X<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p_drop))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dropout_model(x , w_h , w_h2 , w_o , p_drop_input <span class="op">=</span> <span class="fl">0.1</span> , p_drop_hidden <span class="op">=</span> <span class="fl">0.1</span>):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">#apply dropout to X</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    x_drop <span class="op">=</span> dropout(x, p_drop <span class="op">=</span> p_drop_input)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> rectify(x_drop <span class="op">@</span> w_h)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">#apply dropout to h</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    h_drop <span class="op">=</span> dropout(h, p_drop <span class="op">=</span> p_drop_hidden)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    h2 <span class="op">=</span> rectify(h_drop <span class="op">@</span> w_h2)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#apply dropout to h2</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    h2_drop <span class="op">=</span> dropout(h2, p_drop <span class="op">=</span> p_drop_hidden)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    pre_softmax <span class="op">=</span> h2_drop <span class="op">@</span> w_o</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pre_softmax</span></code></pre></div>
</div>
<div class="cell markdown">
<div style="color: green; font-weight: bold">Comment</div>
<p>Bernoulli distribution used instead of binomial.</p>
</div>
<div class="cell code" data-execution_count="24"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;,&quot;height&quot;:1000}"
id="E4krsDO6CAqj" data-outputId="27582878-c74c-4e44-9778-26bb6add116a">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#train model and show data</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">#epochs to train</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># input shape is (B, 784)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>w_h <span class="op">=</span> init_weights((<span class="dv">784</span>, <span class="dv">625</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer with 625 neurons</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>w_h2 <span class="op">=</span> init_weights((<span class="dv">625</span>, <span class="dv">625</span>))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer with 625 neurons</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>w_o <span class="op">=</span> init_weights((<span class="dv">625</span>, <span class="dv">10</span>))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># output shape is (B, 10)</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>intro_params <span class="op">=</span> [w_h, w_h2, w_o]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>intro_train_loss, intro_test_loss <span class="op">=</span> run_model(dropout_model, intro_params, n_epochs)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plot_results(intro_train_loss,intro_test_loss, n_epochs, <span class="st">&quot;with dropout rate of 0.1&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>3
Device: cuda:0
Epoch: 0
Mean Train Loss: 4.51e-01
Mean Test Loss:  3.01e-01
Epoch: 10
Mean Train Loss: 2.42e-01
Mean Test Loss:  2.83e-01
Epoch: 20
Mean Train Loss: 2.11e-01
Mean Test Loss:  3.53e-01
Epoch: 30
Mean Train Loss: 2.21e-01
Mean Test Loss:  6.19e-01
Epoch: 40
Mean Train Loss: 2.13e-01
Mean Test Loss:  8.28e-01
Epoch: 50
Mean Train Loss: 1.99e-01
Mean Test Loss:  7.59e-01
Epoch: 60
Mean Train Loss: 1.98e-01
Mean Test Loss:  8.17e-01
Epoch: 70
Mean Train Loss: 1.70e-01
Mean Test Loss:  9.56e-01
Epoch: 80
Mean Train Loss: 1.85e-01
Mean Test Loss:  8.50e-01
Epoch: 90
Mean Train Loss: 1.70e-01
Mean Test Loss:  9.54e-01
Epoch: 100
Mean Train Loss: 1.62e-01
Mean Test Loss:  7.90e-01
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/b45a1bdb8c9272bf8f4d5bfd11a30a2cbaa7c8c9.png" /></p>
</div>
</div>
<div class="cell markdown">
<div style="color: green; font-weight: bold">Comment</div>
<p>The train and test loss should go up over epochs instead of going
down. The reason that loss goes down here is arributed to not defining
the additional dropout parameters as 0.2</p>
</div>
<div class="cell markdown" id="TJ0uQYgFDfEH">
<ul>
<li><p>Dropout method is used as a regularization technique to reduce
overfitting during training. During training of neural network, a
proporation of input neurons are set to 0 based on given prior
probability or <code>dropout_rate</code>. We are setting those values to
zero thus the work dropping out. It penalizes the model to rely on few
neurons for predictions and thus improving connectivity of neurons
across all neurons to generalize.</p></li>
<li><p>By randomly dropping out units, dropout introduces unpredictable
noise and prevents strong biased connections between few neurons. This
noise forces the neural network to learn redundant features, making it
more robust to variations in the input data. Concequenty, dropout helps
reduce overfitting by preventing the model from memorizing the training
data too closely and improves the model's ability to generalize to
unseen data.</p></li>
<li><p>In test environment, we aim to get reproducible and reliable
predictions. However, dropout introduces noise which in turn generates
randomness during testing. Therefore a new test configuration is
required for testing on unseen data. It includes either removing dropout
layers or disabling them and sometimes use of modified network with
adjusted layers for absence of dropout. By evaluating the model after
dropout layers removal, we can obtain a more reliable and reproducible
estimate of its generalization performance on new, unseen data.</p></li>
<li><p>The difference between training and testing error with dropout
enabled network is significantly lower than in the case of normal
model.</p></li>
<li><p>The test errors for normal model vs dropout model are very
similar and comparable when used for values in 0.1-0.2 range (with
slight improvements with 0.1 or lower value), however, too large value
can cause dropout model to perform worse as found in our
experiments.</p></li>
</ul>
</div>
<section id="task-3-prelu" class="cell markdown" id="TKDRHzwCGPhw">
<h3>Task 3: PReLU</h3>
</section>
<div class="cell code" id="v98E5X3Wuukx">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prelu(x,a):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    zero <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.<span class="bu">max</span>(zero,x) <span class="op">+</span> torch.<span class="bu">min</span>(zero,x)<span class="op">*</span>a</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prelu_model(x, w_h, w_h2, w_o, a):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> prelu(x <span class="op">@</span> w_h, a)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    h2 <span class="op">=</span> prelu(h <span class="op">@</span> w_h2, a)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    pre_softmax <span class="op">=</span> h2 <span class="op">@</span> w_o</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pre_softmax</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize weights</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># input shape is (B, 784)</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>w_h <span class="op">=</span> init_weights((<span class="dv">784</span>, <span class="dv">625</span>))</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer with 625 neurons</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>w_h2 <span class="op">=</span> init_weights((<span class="dv">625</span>, <span class="dv">625</span>))</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># hidden layer with 625 neurons</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>w_o <span class="op">=</span> init_weights((<span class="dv">625</span>, <span class="dv">10</span>))</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co"># output shape is (B, 10)</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> init_weights((<span class="dv">100</span>,<span class="dv">625</span>))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co">#optimizer = RMSprop(params=[w_h, w_h2, w_o])</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [w_h, w_h2, w_o, a]</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>prelu_train_loss, prelu_test_loss <span class="op">=</span> run_model(prelu_model, params, n_epochs)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>plot_results(prelu_train_loss,prelu_test_loss,n_epochs,<span class="st">&quot;prelu&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" id="dzlb9Iypuuky"
data-outputId="b40b5ec5-a81f-4a5e-e29e-b3530e52e6f8">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prelu_train_loss)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[0.43111410001292827, 0.21865547606255858, 0.18132685039852126, 0.16782314110430888]
</code></pre>
</div>
</div>
<div class="cell markdown">
<div style="color: green; font-weight: bold">Comment</div>
<p>the loss goes down over epoch but the test loss is not plotted
here.</p>
</div>
<section id="task-4-convnet" class="cell markdown">
<h1>Task 4 ConvNet</h1>
</section>
<div class="cell code" data-execution_count="2">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> StepLR</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, use_cuda, train_loader, optimizer, epoch, log_interval):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Train one epoch</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">    model -- the neural network</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co">    use_cuda -- true if GPU should be used</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co">    train_loader -- data loader</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co">    optimizer -- network optimizer</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co">    epoch -- number of current epoch</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co">    log_interval -- number of training steps between logs</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: set the model to train mode</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># enumerate over the dataloader to get mini batches</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># of images and ground truth labels</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the builtin python function enumerate() also gives you indices</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (data, target) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">&quot;cuda&quot;</span> <span class="cf">if</span> use_cuda <span class="cf">else</span> <span class="st">&quot;cpu&quot;</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        data, target <span class="op">=</span> data.to(device), target.to(device)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># set the optimizers gradients to zero</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run the network</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> model(data)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute negative log likelihood loss</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.nll_loss(out, target)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># do backpropagation</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># optimize</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print current loss for every nth (&quot;log_interval&quot;th) iteration</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> log_interval <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>          <span class="bu">print</span>(<span class="st">&#39;Epoch: </span><span class="sc">{}</span><span class="st">, Idx: </span><span class="sc">{}</span><span class="st">, Loss: </span><span class="sc">{:.6f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(epoch, i, loss.item()))</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="4">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validate(model, use_cuda, test_loader):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute test metrics</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">    model -- the neural network</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">    use_cuda -- true if GPU should be used</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">    test_loader -- data loader</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># create a 10x10 grid of subplots</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    _, axis <span class="op">=</span> plt.subplots(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set model to evaluation mode</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    plotted <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># disable gradients globally</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (data, target) <span class="kw">in</span> <span class="bu">enumerate</span>(test_loader):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for each batch</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> use_cuda:</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>                <span class="co"># transfer to GPU</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>                data <span class="op">=</span> data.cuda()</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>                target <span class="op">=</span> target.cuda()</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># run network and compute metrics</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> model(data)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> F.nll_loss(output, target, reduction<span class="op">=</span><span class="st">&#39;sum&#39;</span>).item()  <span class="co"># sum up batch loss</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> output.argmax(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)  <span class="co"># get the index of the max log-probability</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            img_correct <span class="op">=</span> pred.eq(target.view_as(pred))</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> pred.eq(target.view_as(pred)).<span class="bu">sum</span>().item()</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            <span class="co"># plot the first 100 images</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            img_idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>            data <span class="op">=</span> data.cpu().numpy()</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> plotted <span class="op">&lt;</span> <span class="dv">100</span> <span class="kw">and</span> img_idx <span class="op">&lt;</span> data.shape[<span class="dv">0</span>]:</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>                <span class="co"># compute position of ith image in the grid</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>                y <span class="op">=</span> plotted <span class="op">%</span> <span class="dv">10</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> plotted <span class="op">//</span> <span class="dv">10</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>                <span class="co"># convert image tensor to numpy array and normalize to [0, 1]</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>                img <span class="op">=</span> data[img_idx, <span class="dv">0</span>]</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>                img <span class="op">=</span> (img <span class="op">-</span> np.<span class="bu">min</span>(img)) <span class="op">/</span> (np.<span class="bu">max</span>(img) <span class="op">-</span> np.<span class="bu">min</span>(img))</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>                <span class="co"># make wrongly predicted images red</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>                img <span class="op">=</span> np.stack([img] <span class="op">*</span> <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> img_correct[img_idx] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>                    img[:, :, <span class="dv">1</span>:] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>                <span class="co"># disable axis and show image</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>                axis[y][x].axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>                axis[y][x].imshow(img)</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>                <span class="co"># show the predicted class next to each image</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>                axis[y][x].text(<span class="dv">30</span>, <span class="dv">25</span>, pred[img_idx].item())</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>                plotted <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>                img_idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> <span class="bu">len</span>(test_loader.dataset)</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># show results</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Test set: Average loss: </span><span class="sc">{:.4f}</span><span class="st">, Accuracy: </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st"> (</span><span class="sc">{:.2f}</span><span class="st">%)</span><span class="ch">\n</span><span class="st">&#39;</span>.<span class="bu">format</span>(</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>        test_loss, correct, <span class="bu">len</span>(test_loader.dataset),</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>        <span class="fl">100.</span> <span class="op">*</span> correct <span class="op">/</span> <span class="bu">len</span>(test_loader.dataset)))</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="5">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FCNet1(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Fully Connected Neural Network</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Five fully connected layers with sigmoid non-linearity</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Dimensions</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">    784-&gt;200-&gt;100-&gt;60-&gt;30-&gt;10</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(FCNet1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize network layers</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>,<span class="dv">200</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> nn.Linear(<span class="dv">200</span>,<span class="dv">100</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin3 <span class="op">=</span> nn.Linear(<span class="dv">100</span>,<span class="dv">60</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin4 <span class="op">=</span> nn.Linear(<span class="dv">60</span>,<span class="dv">30</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin5 <span class="op">=</span> nn.Linear(<span class="dv">30</span>,<span class="dv">10</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape batch of images to batch of 1D vectors</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, start_dim<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run network layers</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin1(x)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.sigmoid(x)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin2(x)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.sigmoid(x)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin3(x)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.sigmoid(x)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin4(x)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.sigmoid(x)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin5(x)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute log softmax over the output</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="6">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FCNet2(nn.Module):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(FCNet2, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>,<span class="dv">200</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> nn.Linear(<span class="dv">200</span>,<span class="dv">100</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin3 <span class="op">=</span> nn.Linear(<span class="dv">100</span>,<span class="dv">60</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin4 <span class="op">=</span> nn.Linear(<span class="dv">60</span>,<span class="dv">30</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin5 <span class="op">=</span> nn.Linear(<span class="dv">30</span>,<span class="dv">10</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape batch of images to batch of 1D vectors</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, start_dim<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run network layers</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin1(x)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin2(x)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin3(x)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin4(x)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin5(x)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute log softmax over the output</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FCNet3(nn.Module):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(FCNet3, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>,<span class="dv">200</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">200</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> nn.Linear(<span class="dv">200</span>,<span class="dv">100</span>)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin3 <span class="op">=</span> nn.Linear(<span class="dv">100</span>,<span class="dv">60</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm3 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">60</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin4 <span class="op">=</span> nn.Linear(<span class="dv">60</span>,<span class="dv">30</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin5 <span class="op">=</span> nn.Linear(<span class="dv">30</span>,<span class="dv">10</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape batch of images to batch of 1D vectors</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, start_dim<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run network layers</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin1(x)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin2(x)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin3(x)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm3(x)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin4(x)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin5(x)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute log softmax over the output</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="8">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet1(nn.Module):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convolutional Neural Network</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Two convolutional layers and two fully connected layers</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Dimensions:</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">    1x28x28-&gt;32x26x26-&gt;64x12x12-&gt;128-&gt;10</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># initialize network layers</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> nn.Linear(<span class="dv">12</span><span class="op">*</span><span class="dv">12</span><span class="op">*</span><span class="dv">64</span>, <span class="dv">128</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run convolutional layers</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># reshape batch of images to batch of 1D vectors</span></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, start_dim<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># run fully connected layers</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin1(x)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin2(x)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute log softmax over the output</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet2(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet2, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">32</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> nn.Linear(<span class="dv">12</span><span class="op">*</span><span class="dv">12</span><span class="op">*</span><span class="dv">64</span>, <span class="dv">128</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, start_dim<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin1(x)</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin2(x)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNet3(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConvNet3, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> nn.BatchNorm2d(<span class="dv">32</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">3</span>) <span class="co"># NEW</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> nn.Linear(<span class="dv">12</span><span class="op">*</span><span class="dv">12</span><span class="op">*</span><span class="dv">64</span>, <span class="dv">128</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.max_pool2d(x, <span class="dv">2</span>) <span class="co"># NEW</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, start_dim<span class="op">=-</span><span class="dv">3</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin1(x)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(x)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.lin2(x)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># hyper parameters</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>test_batch_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>log_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># use GPU if available</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>use_cuda <span class="op">=</span> torch.cuda.is_available()</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>kwargs <span class="op">=</span> {<span class="st">&#39;num_workers&#39;</span>: <span class="dv">1</span>, <span class="st">&#39;pin_memory&#39;</span>: <span class="va">True</span>} <span class="cf">if</span> use_cuda <span class="cf">else</span> {}</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize data loaders</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    datasets.MNIST(<span class="st">&#39;../data&#39;</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(),</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        transforms.Normalize((<span class="fl">0.1307</span>,), (<span class="fl">0.3081</span>,))</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    ])), batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>test_loader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    datasets.MNIST(<span class="st">&#39;../data&#39;</span>, train<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transforms.Compose([</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(), </span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>        transforms.Normalize((<span class="fl">0.1307</span>,), (<span class="fl">0.3081</span>,))</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    ])),</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span>test_batch_size, shuffle<span class="op">=</span><span class="va">True</span>, <span class="op">**</span>kwargs)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> FCNet1()</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> use_cuda:</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.cuda()</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize optimizer and scheduler</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adadelta(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> StepLR(optimizer, step_size<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span>gamma)</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train one epoch</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run on test dataset</span></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    validate(model, use_cuda, test_loader)</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">#torch.save(model.state_dict(), &quot;models/mnist/checkpoint.pt&quot;)</span></span></code></pre></div>
<div class="output stream stderr">
<pre><code>0.3%</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\MNIST\raw\train-images-idx3-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100.0%
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting ../data\MNIST\raw\train-images-idx3-ubyte.gz to ../data\MNIST\raw
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100.0%</code></pre>
</div>
<div class="output stream stdout">
<pre><code>
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\MNIST\raw\train-labels-idx1-ubyte.gz
Extracting ../data\MNIST\raw\train-labels-idx1-ubyte.gz to ../data\MNIST\raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\MNIST\raw\t10k-images-idx3-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100.0%
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting ../data\MNIST\raw\t10k-images-idx3-ubyte.gz to ../data\MNIST\raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\MNIST\raw\t10k-labels-idx1-ubyte.gz
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>100.0%
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Extracting ../data\MNIST\raw\t10k-labels-idx1-ubyte.gz to ../data\MNIST\raw

Epoch: 1, Idx: 0, Loss: 2.303182
Epoch: 1, Idx: 100, Loss: 2.330787
Epoch: 1, Idx: 200, Loss: 2.315116
Epoch: 1, Idx: 300, Loss: 2.304635
Epoch: 1, Idx: 400, Loss: 2.328194
Epoch: 1, Idx: 500, Loss: 2.286837
Epoch: 1, Idx: 600, Loss: 2.295968
Epoch: 1, Idx: 700, Loss: 2.287637
Epoch: 1, Idx: 800, Loss: 2.279447
Epoch: 1, Idx: 900, Loss: 2.135024

Test set: Average loss: 1.8934, Accuracy: 2920/10000 (29.20%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/32d5a78db11b2f0655b2d9077b222e8f1b27fa9c.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 2, Idx: 0, Loss: 1.867714
Epoch: 2, Idx: 100, Loss: 1.496479
Epoch: 2, Idx: 200, Loss: 1.158452
Epoch: 2, Idx: 300, Loss: 0.982584
Epoch: 2, Idx: 400, Loss: 0.976239
Epoch: 2, Idx: 500, Loss: 0.793034
Epoch: 2, Idx: 600, Loss: 0.688842
Epoch: 2, Idx: 700, Loss: 0.779312
Epoch: 2, Idx: 800, Loss: 0.504560
Epoch: 2, Idx: 900, Loss: 0.353043

Test set: Average loss: 0.6143, Accuracy: 8576/10000 (85.76%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/bbb78de2316b52d52fc2578c8d1f58fa68e3cb14.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 3, Idx: 0, Loss: 0.743242
Epoch: 3, Idx: 100, Loss: 0.545265
Epoch: 3, Idx: 200, Loss: 0.748124
Epoch: 3, Idx: 300, Loss: 0.357320
Epoch: 3, Idx: 400, Loss: 0.318961
Epoch: 3, Idx: 500, Loss: 0.393522
Epoch: 3, Idx: 600, Loss: 0.344175
Epoch: 3, Idx: 700, Loss: 0.349083
Epoch: 3, Idx: 800, Loss: 0.212971
Epoch: 3, Idx: 900, Loss: 0.329364

Test set: Average loss: 0.2961, Accuracy: 9344/10000 (93.44%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/216e65e2e67c45a6dcde5a051c38133b690754a8.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 4, Idx: 0, Loss: 0.115937
Epoch: 4, Idx: 100, Loss: 0.275557
Epoch: 4, Idx: 200, Loss: 0.136583
Epoch: 4, Idx: 300, Loss: 0.215620
Epoch: 4, Idx: 400, Loss: 0.230664
Epoch: 4, Idx: 500, Loss: 0.330906
Epoch: 4, Idx: 600, Loss: 0.166897
Epoch: 4, Idx: 700, Loss: 0.116037
Epoch: 4, Idx: 800, Loss: 0.076934
Epoch: 4, Idx: 900, Loss: 0.153353

Test set: Average loss: 0.2678, Accuracy: 9350/10000 (93.50%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/9d6e3a69a1c012c0fb0ebcea3796e8b088779f22.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 5, Idx: 0, Loss: 0.441461
Epoch: 5, Idx: 100, Loss: 0.192070
Epoch: 5, Idx: 200, Loss: 0.067746
Epoch: 5, Idx: 300, Loss: 0.048345
Epoch: 5, Idx: 400, Loss: 0.303630
Epoch: 5, Idx: 500, Loss: 0.125162
Epoch: 5, Idx: 600, Loss: 0.229983
Epoch: 5, Idx: 700, Loss: 0.118895
Epoch: 5, Idx: 800, Loss: 0.074233
Epoch: 5, Idx: 900, Loss: 0.274589

Test set: Average loss: 0.2001, Accuracy: 9529/10000 (95.29%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/f9f13379bd50062bddd6ab73b977fe60e7785264.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 6, Idx: 0, Loss: 0.052316
Epoch: 6, Idx: 100, Loss: 0.139092
Epoch: 6, Idx: 200, Loss: 0.214919
Epoch: 6, Idx: 300, Loss: 0.095604
Epoch: 6, Idx: 400, Loss: 0.143971
Epoch: 6, Idx: 500, Loss: 0.245386
Epoch: 6, Idx: 600, Loss: 0.174834
Epoch: 6, Idx: 700, Loss: 0.269810
Epoch: 6, Idx: 800, Loss: 0.079599
Epoch: 6, Idx: 900, Loss: 0.293781

Test set: Average loss: 0.1922, Accuracy: 9545/10000 (95.45%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/4c8ece09fd4074e7091dfba022809837706d6028.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 7, Idx: 0, Loss: 0.144020
Epoch: 7, Idx: 100, Loss: 0.055795
Epoch: 7, Idx: 200, Loss: 0.053985
Epoch: 7, Idx: 300, Loss: 0.150777
Epoch: 7, Idx: 400, Loss: 0.079593
Epoch: 7, Idx: 500, Loss: 0.268062
Epoch: 7, Idx: 600, Loss: 0.049081
Epoch: 7, Idx: 700, Loss: 0.118911
Epoch: 7, Idx: 800, Loss: 0.104715
Epoch: 7, Idx: 900, Loss: 0.167144

Test set: Average loss: 0.1822, Accuracy: 9573/10000 (95.73%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/3e061c4680e47d026169918aa545acb79b25072e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 8, Idx: 0, Loss: 0.172472
Epoch: 8, Idx: 100, Loss: 0.154866
Epoch: 8, Idx: 200, Loss: 0.042595
Epoch: 8, Idx: 300, Loss: 0.069724
Epoch: 8, Idx: 400, Loss: 0.039705
Epoch: 8, Idx: 500, Loss: 0.074740
Epoch: 8, Idx: 600, Loss: 0.230124
Epoch: 8, Idx: 700, Loss: 0.049027
Epoch: 8, Idx: 800, Loss: 0.041879
Epoch: 8, Idx: 900, Loss: 0.110732

Test set: Average loss: 0.1732, Accuracy: 9602/10000 (96.02%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/d1d84299f1b902b014fddbcfc1ec60c34531ab63.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 9, Idx: 0, Loss: 0.182211
Epoch: 9, Idx: 100, Loss: 0.086931
Epoch: 9, Idx: 200, Loss: 0.049255
Epoch: 9, Idx: 300, Loss: 0.200303
Epoch: 9, Idx: 400, Loss: 0.037545
Epoch: 9, Idx: 500, Loss: 0.059607
Epoch: 9, Idx: 600, Loss: 0.082809
Epoch: 9, Idx: 700, Loss: 0.115031
Epoch: 9, Idx: 800, Loss: 0.033568
Epoch: 9, Idx: 900, Loss: 0.026331

Test set: Average loss: 0.1698, Accuracy: 9607/10000 (96.07%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/8f05fca82b7272c3c058c1a629efeeb3c8f929b7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 10, Idx: 0, Loss: 0.079567
Epoch: 10, Idx: 100, Loss: 0.193416
Epoch: 10, Idx: 200, Loss: 0.053084
Epoch: 10, Idx: 300, Loss: 0.115313
Epoch: 10, Idx: 400, Loss: 0.057696
Epoch: 10, Idx: 500, Loss: 0.098501
Epoch: 10, Idx: 600, Loss: 0.220655
Epoch: 10, Idx: 700, Loss: 0.092480
Epoch: 10, Idx: 800, Loss: 0.118517
Epoch: 10, Idx: 900, Loss: 0.094539

Test set: Average loss: 0.1689, Accuracy: 9615/10000 (96.15%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/884325ad6f3bd8bbeb48a25a12dce2e9e0dd0510.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> FCNet3()</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> use_cuda:</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.cuda()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize optimizer and scheduler</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adadelta(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> StepLR(optimizer, step_size<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span>gamma)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train one epoch</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run on test dataset</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    validate(model, use_cuda, test_loader)</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch: 1, Idx: 0, Loss: 2.328493
Epoch: 1, Idx: 100, Loss: 0.421053
Epoch: 1, Idx: 200, Loss: 0.198695
Epoch: 1, Idx: 300, Loss: 0.217269
Epoch: 1, Idx: 400, Loss: 0.067635
Epoch: 1, Idx: 500, Loss: 0.137915
Epoch: 1, Idx: 600, Loss: 0.186613
Epoch: 1, Idx: 700, Loss: 0.158636
Epoch: 1, Idx: 800, Loss: 0.145484
Epoch: 1, Idx: 900, Loss: 0.100084

Test set: Average loss: 0.1516, Accuracy: 9554/10000 (95.54%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/9944468ca73e38df19c3a8ea783d8588ddaeaa16.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 2, Idx: 0, Loss: 0.276918
Epoch: 2, Idx: 100, Loss: 0.069119
Epoch: 2, Idx: 200, Loss: 0.082194
Epoch: 2, Idx: 300, Loss: 0.085893
Epoch: 2, Idx: 400, Loss: 0.048480
Epoch: 2, Idx: 500, Loss: 0.173318
Epoch: 2, Idx: 600, Loss: 0.070337
Epoch: 2, Idx: 700, Loss: 0.187368
Epoch: 2, Idx: 800, Loss: 0.110332
Epoch: 2, Idx: 900, Loss: 0.073063

Test set: Average loss: 0.0823, Accuracy: 9755/10000 (97.55%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/21e6c9c6f428f4b52474b657a8b4373bf66240bb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 3, Idx: 0, Loss: 0.018660
Epoch: 3, Idx: 100, Loss: 0.073590
Epoch: 3, Idx: 200, Loss: 0.072356
Epoch: 3, Idx: 300, Loss: 0.101509
Epoch: 3, Idx: 400, Loss: 0.050651
Epoch: 3, Idx: 500, Loss: 0.005871
Epoch: 3, Idx: 600, Loss: 0.039137
Epoch: 3, Idx: 700, Loss: 0.025124
Epoch: 3, Idx: 800, Loss: 0.028232
Epoch: 3, Idx: 900, Loss: 0.104676

Test set: Average loss: 0.0651, Accuracy: 9808/10000 (98.08%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/4113ab601892d6cbf6c758862a0a1aa4de4c85f0.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 4, Idx: 0, Loss: 0.012980
Epoch: 4, Idx: 100, Loss: 0.155106
Epoch: 4, Idx: 200, Loss: 0.017121
Epoch: 4, Idx: 300, Loss: 0.045670
Epoch: 4, Idx: 400, Loss: 0.047066
Epoch: 4, Idx: 500, Loss: 0.051705
Epoch: 4, Idx: 600, Loss: 0.003649
Epoch: 4, Idx: 700, Loss: 0.020537
Epoch: 4, Idx: 800, Loss: 0.012965
Epoch: 4, Idx: 900, Loss: 0.026056

Test set: Average loss: 0.0653, Accuracy: 9835/10000 (98.35%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/aac2c2ed9857b920589c90af9ef9af84fc31f3e1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 5, Idx: 0, Loss: 0.034433
Epoch: 5, Idx: 100, Loss: 0.004874
Epoch: 5, Idx: 200, Loss: 0.008345
Epoch: 5, Idx: 300, Loss: 0.193828
Epoch: 5, Idx: 400, Loss: 0.051214
Epoch: 5, Idx: 500, Loss: 0.044604
Epoch: 5, Idx: 600, Loss: 0.005816
Epoch: 5, Idx: 700, Loss: 0.067531
Epoch: 5, Idx: 800, Loss: 0.017431
Epoch: 5, Idx: 900, Loss: 0.008654

Test set: Average loss: 0.0627, Accuracy: 9829/10000 (98.29%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/b7407bc95613745e53d8eb34a4886b9b18daeb98.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 6, Idx: 0, Loss: 0.012718
Epoch: 6, Idx: 100, Loss: 0.006071
Epoch: 6, Idx: 200, Loss: 0.013096
Epoch: 6, Idx: 300, Loss: 0.004132
Epoch: 6, Idx: 400, Loss: 0.067939
Epoch: 6, Idx: 500, Loss: 0.002505
Epoch: 6, Idx: 600, Loss: 0.004779
Epoch: 6, Idx: 700, Loss: 0.004137
Epoch: 6, Idx: 800, Loss: 0.001651
Epoch: 6, Idx: 900, Loss: 0.008712

Test set: Average loss: 0.0713, Accuracy: 9832/10000 (98.32%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/f617579cab8345635db4555b6c7c65c028fdac1f.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 7, Idx: 0, Loss: 0.038677
Epoch: 7, Idx: 100, Loss: 0.047824
Epoch: 7, Idx: 200, Loss: 0.026235
Epoch: 7, Idx: 300, Loss: 0.010894
Epoch: 7, Idx: 400, Loss: 0.001592
Epoch: 7, Idx: 500, Loss: 0.006993
Epoch: 7, Idx: 600, Loss: 0.003202
Epoch: 7, Idx: 700, Loss: 0.018601
Epoch: 7, Idx: 800, Loss: 0.018591
Epoch: 7, Idx: 900, Loss: 0.001025

Test set: Average loss: 0.0652, Accuracy: 9830/10000 (98.30%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/2e37106149e676fba84e1f718b74a3437722932a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 8, Idx: 0, Loss: 0.010245
Epoch: 8, Idx: 100, Loss: 0.026044
Epoch: 8, Idx: 200, Loss: 0.007365
Epoch: 8, Idx: 300, Loss: 0.002690
Epoch: 8, Idx: 400, Loss: 0.022317
Epoch: 8, Idx: 500, Loss: 0.012398
Epoch: 8, Idx: 600, Loss: 0.040314
Epoch: 8, Idx: 700, Loss: 0.002493
Epoch: 8, Idx: 800, Loss: 0.001006
Epoch: 8, Idx: 900, Loss: 0.000811

Test set: Average loss: 0.0678, Accuracy: 9837/10000 (98.37%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/f8b0f11112768b99434aec5a0f917f4e6bee249a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 9, Idx: 0, Loss: 0.002576
Epoch: 9, Idx: 100, Loss: 0.034268
Epoch: 9, Idx: 200, Loss: 0.009190
Epoch: 9, Idx: 300, Loss: 0.002904
Epoch: 9, Idx: 400, Loss: 0.081851
Epoch: 9, Idx: 500, Loss: 0.017628
Epoch: 9, Idx: 600, Loss: 0.001753
Epoch: 9, Idx: 700, Loss: 0.002417
Epoch: 9, Idx: 800, Loss: 0.000842
Epoch: 9, Idx: 900, Loss: 0.001446

Test set: Average loss: 0.0693, Accuracy: 9833/10000 (98.33%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/1463e109b18fae5f89a57a20e375ebbd284fef2a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 10, Idx: 0, Loss: 0.000368
Epoch: 10, Idx: 100, Loss: 0.000258
Epoch: 10, Idx: 200, Loss: 0.002381
Epoch: 10, Idx: 300, Loss: 0.011220
Epoch: 10, Idx: 400, Loss: 0.000993
Epoch: 10, Idx: 500, Loss: 0.024164
Epoch: 10, Idx: 600, Loss: 0.005922
Epoch: 10, Idx: 700, Loss: 0.017190
Epoch: 10, Idx: 800, Loss: 0.002297
Epoch: 10, Idx: 900, Loss: 0.001531

Test set: Average loss: 0.0713, Accuracy: 9835/10000 (98.35%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/672bafc075e9bb7951490bfb76d48eddff4cc406.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNet1()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> use_cuda:</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.cuda()</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize optimizer and scheduler</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adadelta(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> StepLR(optimizer, step_size<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span>gamma)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train one epoch</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run on test dataset</span></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>    validate(model, use_cuda, test_loader)</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch: 1, Idx: 0, Loss: 2.293617
Epoch: 1, Idx: 100, Loss: 0.128400
Epoch: 1, Idx: 200, Loss: 0.134073
Epoch: 1, Idx: 300, Loss: 0.086571
Epoch: 1, Idx: 400, Loss: 0.008691
Epoch: 1, Idx: 500, Loss: 0.051266
Epoch: 1, Idx: 600, Loss: 0.070458
Epoch: 1, Idx: 700, Loss: 0.003735
Epoch: 1, Idx: 800, Loss: 0.253935
Epoch: 1, Idx: 900, Loss: 0.038215

Test set: Average loss: 0.0436, Accuracy: 9849/10000 (98.49%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/558cf3c9166d44d2c81c085e1564dc68f29fea17.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 2, Idx: 0, Loss: 0.021624
Epoch: 2, Idx: 100, Loss: 0.051011
Epoch: 2, Idx: 200, Loss: 0.014701
Epoch: 2, Idx: 300, Loss: 0.004543
Epoch: 2, Idx: 400, Loss: 0.082584
Epoch: 2, Idx: 500, Loss: 0.108714
Epoch: 2, Idx: 600, Loss: 0.001811
Epoch: 2, Idx: 700, Loss: 0.026595
Epoch: 2, Idx: 800, Loss: 0.005317
Epoch: 2, Idx: 900, Loss: 0.013590

Test set: Average loss: 0.0350, Accuracy: 9886/10000 (98.86%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/bf4a0f69a9bd86ea420c37e09f2471f9e684827d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 3, Idx: 0, Loss: 0.086989
Epoch: 3, Idx: 100, Loss: 0.084507
Epoch: 3, Idx: 200, Loss: 0.001366
Epoch: 3, Idx: 300, Loss: 0.000260
Epoch: 3, Idx: 400, Loss: 0.000889
Epoch: 3, Idx: 500, Loss: 0.000932
Epoch: 3, Idx: 600, Loss: 0.006685
Epoch: 3, Idx: 700, Loss: 0.001503
Epoch: 3, Idx: 800, Loss: 0.001372
Epoch: 3, Idx: 900, Loss: 0.001287

Test set: Average loss: 0.0283, Accuracy: 9910/10000 (99.10%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/91f6968f6d7097cc12ae2a664e01694ae8ab491d.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 4, Idx: 0, Loss: 0.001693
Epoch: 4, Idx: 100, Loss: 0.000420
Epoch: 4, Idx: 200, Loss: 0.003847
Epoch: 4, Idx: 300, Loss: 0.000100
Epoch: 4, Idx: 400, Loss: 0.012069
Epoch: 4, Idx: 500, Loss: 0.001704
Epoch: 4, Idx: 600, Loss: 0.000263
Epoch: 4, Idx: 700, Loss: 0.003662
Epoch: 4, Idx: 800, Loss: 0.001533
Epoch: 4, Idx: 900, Loss: 0.004566

Test set: Average loss: 0.0313, Accuracy: 9899/10000 (98.99%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/f9c484d9301fc78f5d414800ec0b1704c74cce03.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 5, Idx: 0, Loss: 0.000355
Epoch: 5, Idx: 100, Loss: 0.000456
Epoch: 5, Idx: 200, Loss: 0.000458
Epoch: 5, Idx: 300, Loss: 0.000045
Epoch: 5, Idx: 400, Loss: 0.003693
Epoch: 5, Idx: 500, Loss: 0.039269
Epoch: 5, Idx: 600, Loss: 0.000105
Epoch: 5, Idx: 700, Loss: 0.000195
Epoch: 5, Idx: 800, Loss: 0.000120
Epoch: 5, Idx: 900, Loss: 0.000176

Test set: Average loss: 0.0321, Accuracy: 9905/10000 (99.05%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/149c693682d6adaca51664d86b3965945ab9af53.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 6, Idx: 0, Loss: 0.000182
Epoch: 6, Idx: 100, Loss: 0.000460
Epoch: 6, Idx: 200, Loss: 0.000079
Epoch: 6, Idx: 300, Loss: 0.007645
Epoch: 6, Idx: 400, Loss: 0.000043
Epoch: 6, Idx: 500, Loss: 0.000126
Epoch: 6, Idx: 600, Loss: 0.000253
Epoch: 6, Idx: 700, Loss: 0.000596
Epoch: 6, Idx: 800, Loss: 0.000643
Epoch: 6, Idx: 900, Loss: 0.002598

Test set: Average loss: 0.0339, Accuracy: 9907/10000 (99.07%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/1a2152a02ca7d1da85ea7d22de68eb5be835895e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 7, Idx: 0, Loss: 0.000330
Epoch: 7, Idx: 100, Loss: 0.000353
Epoch: 7, Idx: 200, Loss: 0.000062
Epoch: 7, Idx: 300, Loss: 0.003264
Epoch: 7, Idx: 400, Loss: 0.000354
Epoch: 7, Idx: 500, Loss: 0.000070
Epoch: 7, Idx: 600, Loss: 0.000042
Epoch: 7, Idx: 700, Loss: 0.000557
Epoch: 7, Idx: 800, Loss: 0.000336
Epoch: 7, Idx: 900, Loss: 0.000015

Test set: Average loss: 0.0358, Accuracy: 9908/10000 (99.08%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/9f4117ccf90012449202f09a6e8bb8ac9f79f9c6.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 8, Idx: 0, Loss: 0.000016
Epoch: 8, Idx: 100, Loss: 0.000023
Epoch: 8, Idx: 200, Loss: 0.000141
Epoch: 8, Idx: 300, Loss: 0.000028
Epoch: 8, Idx: 400, Loss: 0.000299
Epoch: 8, Idx: 500, Loss: 0.000211
Epoch: 8, Idx: 600, Loss: 0.003435
Epoch: 8, Idx: 700, Loss: 0.000432
Epoch: 8, Idx: 800, Loss: 0.000401
Epoch: 8, Idx: 900, Loss: 0.000110

Test set: Average loss: 0.0369, Accuracy: 9906/10000 (99.06%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/8008334a6dd35e48653489e0e859c3fca63526fb.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 9, Idx: 0, Loss: 0.000782
Epoch: 9, Idx: 100, Loss: 0.000005
Epoch: 9, Idx: 200, Loss: 0.000374
Epoch: 9, Idx: 300, Loss: 0.000393
Epoch: 9, Idx: 400, Loss: 0.000030
Epoch: 9, Idx: 500, Loss: 0.000029
Epoch: 9, Idx: 600, Loss: 0.000174
Epoch: 9, Idx: 700, Loss: 0.000002
Epoch: 9, Idx: 800, Loss: 0.000367
Epoch: 9, Idx: 900, Loss: 0.000002

Test set: Average loss: 0.0374, Accuracy: 9906/10000 (99.06%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/30c8b0f44db4108d1578f041cbe04bfc641be80e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 10, Idx: 0, Loss: 0.008155
Epoch: 10, Idx: 100, Loss: 0.000487
Epoch: 10, Idx: 200, Loss: 0.000013
Epoch: 10, Idx: 300, Loss: 0.000888
Epoch: 10, Idx: 400, Loss: 0.000063
Epoch: 10, Idx: 500, Loss: 0.000638
Epoch: 10, Idx: 600, Loss: 0.000511
Epoch: 10, Idx: 700, Loss: 0.000010
Epoch: 10, Idx: 800, Loss: 0.000032
Epoch: 10, Idx: 900, Loss: 0.000066

Test set: Average loss: 0.0378, Accuracy: 9905/10000 (99.05%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/a2c4c9cfb1b3c0223942950949f46bd6d71ee5b8.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="14">
<div class="sourceCode" id="cb65"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNet3()</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> use_cuda:</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.cuda()</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize optimizer and scheduler</span></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adadelta(model.parameters(), lr<span class="op">=</span>lr)</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> StepLR(optimizer, step_size<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span>gamma)</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># train one epoch</span></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># run on test dataset</span></span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>    validate(model, use_cuda, test_loader)</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch: 1, Idx: 0, Loss: 2.360789
Epoch: 1, Idx: 100, Loss: 0.132752
Epoch: 1, Idx: 200, Loss: 0.102224
Epoch: 1, Idx: 300, Loss: 0.296614
Epoch: 1, Idx: 400, Loss: 0.128925
Epoch: 1, Idx: 500, Loss: 0.100123
Epoch: 1, Idx: 600, Loss: 0.029676
Epoch: 1, Idx: 700, Loss: 0.009975
Epoch: 1, Idx: 800, Loss: 0.057376
Epoch: 1, Idx: 900, Loss: 0.016293

Test set: Average loss: 0.0396, Accuracy: 9870/10000 (98.70%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/0b5378a8014dbb8b6c834d175457c1ba2f6dd299.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 2, Idx: 0, Loss: 0.015049
Epoch: 2, Idx: 100, Loss: 0.005793
Epoch: 2, Idx: 200, Loss: 0.017739
Epoch: 2, Idx: 300, Loss: 0.032165
Epoch: 2, Idx: 400, Loss: 0.096393
Epoch: 2, Idx: 500, Loss: 0.013347
Epoch: 2, Idx: 600, Loss: 0.003267
Epoch: 2, Idx: 700, Loss: 0.008429
Epoch: 2, Idx: 800, Loss: 0.036682
Epoch: 2, Idx: 900, Loss: 0.005017

Test set: Average loss: 0.0329, Accuracy: 9897/10000 (98.97%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/55fc6d54830a23e6a5b42bdf18940d071230c758.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 3, Idx: 0, Loss: 0.025796
Epoch: 3, Idx: 100, Loss: 0.025752
Epoch: 3, Idx: 200, Loss: 0.053219
Epoch: 3, Idx: 300, Loss: 0.032609
Epoch: 3, Idx: 400, Loss: 0.072766
Epoch: 3, Idx: 500, Loss: 0.052133
Epoch: 3, Idx: 600, Loss: 0.122656
Epoch: 3, Idx: 700, Loss: 0.040073
Epoch: 3, Idx: 800, Loss: 0.000759
Epoch: 3, Idx: 900, Loss: 0.000472

Test set: Average loss: 0.0290, Accuracy: 9915/10000 (99.15%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/a13066591f4e666a519d6cc778b69489c777958a.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 4, Idx: 0, Loss: 0.007341
Epoch: 4, Idx: 100, Loss: 0.000665
Epoch: 4, Idx: 200, Loss: 0.000909
Epoch: 4, Idx: 300, Loss: 0.013384
Epoch: 4, Idx: 400, Loss: 0.002075
Epoch: 4, Idx: 500, Loss: 0.005209
Epoch: 4, Idx: 600, Loss: 0.007327
Epoch: 4, Idx: 700, Loss: 0.001287
Epoch: 4, Idx: 800, Loss: 0.000166
Epoch: 4, Idx: 900, Loss: 0.014199

Test set: Average loss: 0.0284, Accuracy: 9919/10000 (99.19%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/55f26dccbcc8219a9f1915972859226134c5b81e.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 5, Idx: 0, Loss: 0.001463
Epoch: 5, Idx: 100, Loss: 0.013761
Epoch: 5, Idx: 200, Loss: 0.002809
Epoch: 5, Idx: 300, Loss: 0.001835
Epoch: 5, Idx: 400, Loss: 0.002681
Epoch: 5, Idx: 500, Loss: 0.007898
Epoch: 5, Idx: 600, Loss: 0.000310
Epoch: 5, Idx: 700, Loss: 0.001431
Epoch: 5, Idx: 800, Loss: 0.000208
Epoch: 5, Idx: 900, Loss: 0.000211

Test set: Average loss: 0.0273, Accuracy: 9919/10000 (99.19%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/22735fd9c8cfd37da14fc6aa47d1068857a36daa.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 6, Idx: 0, Loss: 0.001662
Epoch: 6, Idx: 100, Loss: 0.001022
Epoch: 6, Idx: 200, Loss: 0.000558
Epoch: 6, Idx: 300, Loss: 0.001614
Epoch: 6, Idx: 400, Loss: 0.000355
Epoch: 6, Idx: 500, Loss: 0.002352
Epoch: 6, Idx: 600, Loss: 0.002541
Epoch: 6, Idx: 700, Loss: 0.000468
Epoch: 6, Idx: 800, Loss: 0.001359
Epoch: 6, Idx: 900, Loss: 0.000592

Test set: Average loss: 0.0274, Accuracy: 9925/10000 (99.25%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/e8c05bf6e04df4680447901a957558b3d3d8c125.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 7, Idx: 0, Loss: 0.000346
Epoch: 7, Idx: 100, Loss: 0.000385
Epoch: 7, Idx: 200, Loss: 0.000656
Epoch: 7, Idx: 300, Loss: 0.000314
Epoch: 7, Idx: 400, Loss: 0.002370
Epoch: 7, Idx: 500, Loss: 0.001913
Epoch: 7, Idx: 600, Loss: 0.002891
Epoch: 7, Idx: 700, Loss: 0.002696
Epoch: 7, Idx: 800, Loss: 0.001094
Epoch: 7, Idx: 900, Loss: 0.000117

Test set: Average loss: 0.0283, Accuracy: 9921/10000 (99.21%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/09c0abfcc7329a725ec1b639cc8d41bd4bd271e1.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 8, Idx: 0, Loss: 0.000418
Epoch: 8, Idx: 100, Loss: 0.000169
Epoch: 8, Idx: 200, Loss: 0.000055
Epoch: 8, Idx: 300, Loss: 0.001977
Epoch: 8, Idx: 400, Loss: 0.000160
Epoch: 8, Idx: 500, Loss: 0.001324
Epoch: 8, Idx: 600, Loss: 0.003789
Epoch: 8, Idx: 700, Loss: 0.000368
Epoch: 8, Idx: 800, Loss: 0.000853
Epoch: 8, Idx: 900, Loss: 0.000701

Test set: Average loss: 0.0285, Accuracy: 9918/10000 (99.18%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/1490517bb53000951af79010032865214871bec7.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 9, Idx: 0, Loss: 0.001074
Epoch: 9, Idx: 100, Loss: 0.000639
Epoch: 9, Idx: 200, Loss: 0.001297
Epoch: 9, Idx: 300, Loss: 0.000077
Epoch: 9, Idx: 400, Loss: 0.000681
Epoch: 9, Idx: 500, Loss: 0.000446
Epoch: 9, Idx: 600, Loss: 0.000372
Epoch: 9, Idx: 700, Loss: 0.000955
Epoch: 9, Idx: 800, Loss: 0.000170
Epoch: 9, Idx: 900, Loss: 0.000448

Test set: Average loss: 0.0283, Accuracy: 9915/10000 (99.15%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/c4cea0f39d8c60363454e6b1105d2d4e7abe0655.png" /></p>
</div>
<div class="output stream stdout">
<pre><code>Epoch: 10, Idx: 0, Loss: 0.000205
Epoch: 10, Idx: 100, Loss: 0.000318
Epoch: 10, Idx: 200, Loss: 0.000855
Epoch: 10, Idx: 300, Loss: 0.002900
Epoch: 10, Idx: 400, Loss: 0.000274
Epoch: 10, Idx: 500, Loss: 0.000385
Epoch: 10, Idx: 600, Loss: 0.001096
Epoch: 10, Idx: 700, Loss: 0.000616
Epoch: 10, Idx: 800, Loss: 0.000867
Epoch: 10, Idx: 900, Loss: 0.000727

Test set: Average loss: 0.0290, Accuracy: 9923/10000 (99.23%)

</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_947fa6108e6f4e5bac2b283f1d51ffca/1f9071277da1f1ccdf19dffe23529e64a44373bb.png" /></p>
</div>
</div>
<div class="cell markdown">
<div style="color: green; font-weight: bold">Comment</div>
<p>The loss is not plotted. unnessarily long solution. Image filtered
with different filter not plotted . visualizatio is different than asked
in the question weights are differently initialiyed. no input and hidden
dropout rates.</p>
</div>
<div class="cell code">
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
</body>
</html>
