{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6de6be1",
   "metadata": {},
   "source": [
    "## Comments for Question 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce713",
   "metadata": {},
   "source": [
    "Write comment from looking into PDF\n",
    "1. Logical OR:<br>\n",
    "Our solution is exactly similar to the given solution in sample solution pdf.\n",
    "2. Masked OR:<br>\n",
    "We use same activation function for predicting the output, Stepwise but with slightly different weights, the bias need to adjusted accordingly to account for prediction. Our solution works perfectly for binary input.\n",
    "3. Fixed Match: </br>\n",
    "We have used a different activation function and weights method, however for a binary input with fixed arbitrary vector, our solution works absolutely correctly. To use with higher order inputs we need to update the weights vector according to the given sample solution. \n",
    "\n",
    "\n",
    "<br>\n",
    "Please write feeedback for part 2 of que 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202192a0",
   "metadata": {},
   "source": [
    "## Comments for Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f6edb",
   "metadata": {},
   "source": [
    "The solution is written in PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b09b7",
   "metadata": {},
   "source": [
    "## Comments for Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88fec12",
   "metadata": {},
   "source": [
    "Since we have written our question 3 solution as a .py file, we will paste our solution in a single cell followed by our self-feedback in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8fe849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.1785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "####################################\n",
    "\n",
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0,input) # your code here\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        downstream_gradient = upstream_gradient * (self.input > 0) # your code here\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        e_x = np.exp(input - np.max(input))\n",
    "        return e_x / e_x.sum() # your code here\n",
    "        \n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture) \n",
    "        #print(predicted_posteriors.shape, self.input.shape, true_labels.shape)\n",
    "        #one hot encode true_labels\n",
    "        order = np.array(list(set(true_labels)))\n",
    "        one_hot = np.eye(len(order))\n",
    "        true_labels = one_hot[order[true_labels]]\n",
    "        downstream_gradient = predicted_posteriors * (self.input - true_labels)\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free\n",
    "\n",
    "####################################\n",
    "\n",
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.weights = np.random.normal(size = (n_inputs, n_outputs)) # your code here\n",
    "        self.bias = np.random.normal(size = (n_outputs)) # your code here\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations = np.dot(input,self.weights) + self.bias # your code here\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        \n",
    "        self.grad_weights = np.matmul(self.input.T,upstream_gradient)\n",
    "        self.grad_bias = np.sum(upstream_gradient, axis=0, keepdims=True)\n",
    "        downstream_gradient = np.dot(upstream_gradient,self.weights.T)\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        \n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.weights = self.weights - learning_rate * self.grad_weights\n",
    "        self.bias = self.bias - learning_rate * self.grad_bias\n",
    "\n",
    "####################################\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        batch_size = predicted_posteriors.shape[0]\n",
    "\n",
    "        # Compute the gradient of the output layer using the predicted posteriors and true classes\n",
    "        upstream_gradient = self.layers[-1].backward(predicted_posteriors,true_classes)\n",
    "\n",
    "        # Backpropagate the gradients through the layers\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            upstream_gradient = layer.backward(upstream_gradient)\n",
    "\n",
    "        return upstream_gradient\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs-1):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)\n",
    "\n",
    "##################################\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # set training/test set size\n",
    "    N = 2000\n",
    "\n",
    "    # create training and test data\n",
    "    X_train, Y_train = datasets.make_moons(N, noise=0.05)\n",
    "    X_test,  Y_test  = datasets.make_moons(N, noise=0.05)\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "\n",
    "    # standardize features to be in [-1, 1]\n",
    "    offset  = X_train.min(axis=0)\n",
    "    scaling = X_train.max(axis=0) - offset\n",
    "    X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "    X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "\n",
    "    # set hyperparameters (play with these!)\n",
    "    layer_sizes = [5, 5, n_classes]\n",
    "    n_epochs = 5\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.05\n",
    "\n",
    "    # create network\n",
    "    network = MLP(n_features, layer_sizes)\n",
    "\n",
    "    # train\n",
    "    network.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "    # test\n",
    "    predicted_posteriors = network.forward(X_test)\n",
    "    # determine class predictions from posteriors by winner-takes-all rule\n",
    "    \n",
    "    predicted_classes =  np.argmax(predicted_posteriors,axis=1)# your code here\n",
    "    # compute and output the error rate of predicted_classes\n",
    "    error_rate = sum(predicted_classes != Y_test)/len(Y_test) # your code here\n",
    "    print(\"error rate:\", error_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e98cc3",
   "metadata": {},
   "source": [
    "* In the ReLU Layer backward function, we could have used negative values of input to calculate downstream gradients instead of positive.\n",
    "* We can improve the clarity of input forward pass in OutputLayer by specifiying the dimension to do the summation over. \n",
    "* We have complete and correct implementation for LinearLayer and MLP class. Both of them works absolutely correctly and most of the method is similar to show in the sample solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5b5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
