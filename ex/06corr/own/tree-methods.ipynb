{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import scipy\n",
    "from abc import abstractmethod\n",
    "from math import log, e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "        \n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min # your code here\n",
    "        \n",
    "\n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        return np.random.choice(D, size=D_try, replace=False) # your code here\n",
    "        \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        # sort the feature values along feature j\n",
    "        sorted_features = np.sort(node.features[:, j])\n",
    "\n",
    "        # find the candidate thresholds (in the middle between adjacent feature values)\n",
    "        thresholds = (sorted_features[:-1] + sorted_features[1:]) / 2\n",
    "\n",
    "        return thresholds # your code here\n",
    "       \n",
    "               \n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        # your code here\n",
    "        # split the data according to the threshold t\n",
    "        left_mask = node.features[:, j] <= t\n",
    "        right_mask = node.features[:, j] > t\n",
    "\n",
    "        # assign the instances to the children\n",
    "        left.features = node.features[left_mask]\n",
    "        left.responses = node.responses[left_mask]\n",
    "        right.features = node.features[right_mask]\n",
    "        right.responses = node.responses[right_mask]\n",
    "\n",
    "        return left, right \n",
    "        \n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> slightly different from solution, especially different threshhold chosen </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        \n",
    "        # your code here\n",
    "        #compute loss\n",
    "        left_mask = node.features[:, j] <= t\n",
    "        right_mask = node.features[:, j] > t\n",
    "\n",
    "        # check if the split is admissible\n",
    "        if np.sum(left_mask) < self.n_min or np.sum(right_mask) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        # compute the loss for the split\n",
    "        left_responses = node.responses[left_mask]\n",
    "        right_responses = node.responses[right_mask]\n",
    "\n",
    "        # squared loss for regression\n",
    "        left_loss = np.mean((left_responses - np.mean(left_responses)) ** 2)\n",
    "        right_loss = np.mean((right_responses - np.mean(right_responses)) ** 2)\n",
    "        loss = left_loss + right_loss\n",
    "\n",
    "        return loss\n",
    "       \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        node.prediction =  np.mean(node.responses) # your code here\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> different from solution also works </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        self.criterion = 'entropy' # or 'gini'\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        # your code here\n",
    "        left_mask = node.features[:, j] <= t\n",
    "        right_mask = node.features[:, j] > t\n",
    "\n",
    "        # check if the split is admissible\n",
    "        if np.sum(left_mask) < self.n_min or np.sum(right_mask) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        # compute the loss for the split\n",
    "        left_responses = node.responses[left_mask]\n",
    "        right_responses = node.responses[right_mask]\n",
    "\n",
    "        left_p = np.mean(left_responses == 1)\n",
    "        right_p = np.mean(right_responses == 1)\n",
    "        left_loss = self.entropy(left_p) if self.criterion == 'entropy' else self.gini_impurity(left_p)\n",
    "        right_loss = self.entropy(right_p) if self.criterion == 'entropy' else self.gini_impurity(right_p)\n",
    "        loss = (np.sum(left_mask) * left_loss + np.sum(right_mask) * right_loss) / node.features.shape[0]\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def entropy(self, p):\n",
    "        ''' compute the entropy\n",
    "        '''\n",
    "        value,counts = np.unique(p, return_counts=True)\n",
    "        norm_counts = counts / counts.sum()\n",
    "        base = e\n",
    "        return -(norm_counts * np.log(norm_counts)/np.log(base)).sum()\n",
    "    \n",
    "    def gini_impurity(self, p):\n",
    "        ''' compute the Gini impurity\n",
    "        '''\n",
    "        return 1 - p ** 2 - (1 - p) ** 2\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        # your code here\n",
    "        #considering the case where there are more than 2 classes class labels as well, the minlengh of bin\n",
    "        counts_bins = np.bincount(node.responses, minlength=max(self.classes) + 1)\n",
    "        # print(counts_bins)\n",
    "        pred_counts = counts_bins[self.classes]  #only select indices that are in self.classes (others are zero)\n",
    "        # print(np.argmax(pred_counts), pred_counts)\n",
    "        node.prediction = self.classes[np.argmax(pred_counts)]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> different from solution also works </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean MSE score for RegressionTree on cross validation is:  0.4131528957727181\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "# your code here\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation(model, features, responses, n_splits=5, regression = False):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        model.train(features[train_index], responses[train_index])\n",
    "        preds = np.array([model.predict(features[t]) for t in test_index])\n",
    "        if regression:\n",
    "            #for regression we calculate MSE\n",
    "            scores.append(np.sum((preds - responses[test_index]) ** 2) / len(responses[test_index]))\n",
    "        else:\n",
    "            #calculate accuracy from preds and responses for classification\n",
    "            scores.append(np.sum((preds == responses[test_index]))/len(responses[test_index]))\n",
    "    # print(scores)\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"The mean MSE score for RegressionTree on cross validation is: \", cross_validation(RegressionTree(n_min=5), features, responses, regression=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> MSE in solution is smaller and more detailed </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score for ClassificationTree on cross validation is:  0.7798325722983257\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# your code here\n",
    "# print(np.unique(labels))\n",
    "print(\"The mean accuracy score for ClassificationTree on cross validation is: \",cross_validation(ClassificationTree(classes=np.unique(labels)), features, labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> MSE in solution is much smaller and more detailed </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses, split_by_response=None):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "\n",
    "    if split_by_response is None:\n",
    "        # determine the number of data points\n",
    "        num_data_points = len(features)\n",
    "\n",
    "        # generate a set of random indices with replacement\n",
    "        bootstrap_indices = np.random.choice(range(num_data_points), size=num_data_points, replace=True)\n",
    "\n",
    "        # use these indices to create a bootstrap sample of the original dataset\n",
    "        bootstrap_features = features[bootstrap_indices]\n",
    "        bootstrap_responses = responses[bootstrap_indices]\n",
    "    else:\n",
    "        # split the data set based on the supplied response\n",
    "        features_target = features[responses == split_by_response]\n",
    "        responses_target = responses[responses == split_by_response]\n",
    "\n",
    "        features_off_target = features[responses != split_by_response]\n",
    "        responses_off_target = responses[responses != split_by_response]\n",
    "\n",
    "        # determine the number of data points\n",
    "        num_target_points = len(features_target)\n",
    "        num_off_target_points = len(features_off_target)\n",
    "        \n",
    "        # generate a set of random indices with replacement\n",
    "        bootstrap_indices_target = np.random.choice(range(num_target_points), size=num_target_points, replace=True)\n",
    "        bootstrap_indices_off_target = np.random.choice(range(num_off_target_points), size=num_target_points, replace=True)\n",
    "        \n",
    "        # use these indices to create a bootstrap sample of the original dataset\n",
    "        bootstrap_features = np.concatenate((features_target[bootstrap_indices_target], features_off_target[bootstrap_indices_off_target]))\n",
    "        bootstrap_responses = np.concatenate((responses_target[bootstrap_indices_target], responses_off_target[bootstrap_indices_off_target]))\n",
    "        bootstrap_responses = np.array([1 if r == split_by_response else -1 for r in bootstrap_responses])\n",
    "\n",
    "    return bootstrap_features, bootstrap_responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> solution much shorter</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        ... # your code here\n",
    "        predictions = [tree.predict(x) for tree in self.trees]\n",
    "\n",
    "        index_max = -1\n",
    "        score_max = -1\n",
    "        for index, score in enumerate(predictions):\n",
    "            if score > score_max:\n",
    "                index_max = index\n",
    "                score_max = score\n",
    "\n",
    "        if score_max > 0:\n",
    "            return self.classes[index_max]\n",
    "        else:\n",
    "            return \"unknown\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> solution much shorter </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        ... # your code here\n",
    "        predictions = [tree.predict(x) for tree in self.trees]\n",
    "        return scipy.stats.mode(predictions, axis=None, keepdims=False)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> different from solution </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RegressionForest' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# using RegressionForest(n_trees=10)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# and comment on your results\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# your code here\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m cross_val_score \u001b[39m=\u001b[39m cross_validation(RegressionForest(n_trees\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, n_min\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m), features, responses, regression\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe mean MSE score for RegressionForest on cross validation is: \u001b[39m\u001b[39m\"\u001b[39m, cross_val_score)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(model, features, responses, n_splits, regression)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(features):\n\u001b[1;32m     11\u001b[0m     model\u001b[39m.\u001b[39mtrain(features[train_index], responses[train_index])\n\u001b[0;32m---> 12\u001b[0m     preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model\u001b[39m.\u001b[39mpredict(features[t]) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m test_index])\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m regression:\n\u001b[1;32m     14\u001b[0m         \u001b[39m#for regression we calculate MSE\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         scores\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39msum((preds \u001b[39m-\u001b[39m responses[test_index]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(responses[test_index]))\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m train_index, test_index \u001b[39min\u001b[39;00m kf\u001b[39m.\u001b[39msplit(features):\n\u001b[1;32m     11\u001b[0m     model\u001b[39m.\u001b[39mtrain(features[train_index], responses[train_index])\n\u001b[0;32m---> 12\u001b[0m     preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model\u001b[39m.\u001b[39;49mpredict(features[t]) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m test_index])\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m regression:\n\u001b[1;32m     14\u001b[0m         \u001b[39m#for regression we calculate MSE\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         scores\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39msum((preds \u001b[39m-\u001b[39m responses[test_index]) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(responses[test_index]))\n",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m, in \u001b[0;36mRegressionForest.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         score_max \u001b[39m=\u001b[39m score\n\u001b[1;32m     23\u001b[0m \u001b[39mif\u001b[39;00m score_max \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses[index_max]\n\u001b[1;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RegressionForest' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "# your code here\n",
    "\n",
    "cross_val_score = cross_validation(RegressionForest(n_trees=10, n_min=5), features, responses, regression=True)\n",
    "print(\"The mean MSE score for RegressionForest on cross validation is: \", cross_val_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> Failed due to attibute error </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score for ClassificationTree on cross validation is:  0.917503805175038\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# your code here\n",
    "\n",
    "cross_val_score = cross_validation(ClassificationForest(n_trees=10, classes=np.unique(labels)), features, labels)\n",
    "\n",
    "print(\"The mean accuracy score for ClassificationTree on cross validation is: \",cross_val_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> MSE in solution is smaller and more detailed </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAGZCAYAAAA6vW6MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKk0lEQVR4nO3deVxUVf8H8M8FZECWQVC2REAscdfcUszlCXHfzSwsxLXEBc1Sf4ULpmTlkma4lGsuuSSllUUomokbpI8r7koqoCGgGIvM+f3hw+QIKOMMc7nO5/163Vdx5957vmfA+c4599xzJCGEABERkQJYyB0AERFRWTFpERGRYjBpERGRYjBpERGRYjBpERGRYjBpERGRYjBpERGRYjBpERGRYjBpERGRYigyaZ07dw5BQUFQq9WQJAkxMTFGvf7ly5chSRJWrVpl1Os+C3x8fDB48GCjXS8tLQ39+/eHi4sLJEnCggULjHZtOej7tyNJEqZPn65XGfHx8ZAkCfHx8XrHV1FUhDqU9N4fPnwYrVu3hp2dHSRJwtGjRzF9+nRIkmTy+Pg5VLKnTloXLlzAyJEjUbNmTdjY2MDR0REBAQH4/PPP8c8//xgzxmJCQkJw/PhxzJo1C2vXrkWzZs3Ktbxn0alTpzB9+nRcvnxZ1jjGjx+PX375BVOmTMHatWvRuXPnci1PkiRIkoRhw4aV+PoHH3ygPebWrVtGKfOnn37SOzEp2bZt29ClSxdUrVoV1tbW8PT0xIABA7Br1y65Q3usgoICvPrqq8jIyMD8+fOxdu1aeHt7l3u569evV/yXNZMST2HHjh3C1tZWODk5ibFjx4ply5aJL774QgwcOFBUqlRJDB8+/GkuWyb37t0TAMQHH3xQbmVoNBrxzz//iPv375dbGXLbvHmzACB2796t13m5ubkiPz/faHG4ubmJ4OBgo13vSQAIGxsb4eTkJPLy8oq97uvrK2xsbAQAcfPmTb2vf+nSJQFArFy5UrsvLCxMlPZP7Z9//hEFBQV6lbF79+6n+t2VN41GIwYPHiwAiCZNmohZs2aJr7/+Wnz00UeiadOmAoD4448/hBAVow6PvvenT58WAMTy5ct1jisoKBD//PNPucXRrVs34e3tXWy/OXwOPQ0rfZPcpUuXMHDgQHh7e2PXrl3w8PDQvhYWFobz58/jxx9/NDybluLmzZsAACcnp3IrQ5Ik2NjYlNv1lUYIgdzcXNja2kKlUhn12unp6Ub9Xebm5sLa2hoWFqV3InTu3Bk//PADfv75Z/Tq1Uu7f//+/bh06RL69euHrVu3Gi2mx3mW/s7mzp2LVatWITw8HPPmzdPpUvvggw+wdu1aWFnp/ZFTbh5979PT0wEU/2yxsrKSJW5+DpVC3yz39ttv63xjepKCggIRGRkpatasKaytrYW3t7eYMmWKyM3N1TnO29tbdOvWTfz++++iefPmQqVSCV9fX7F69WrtMdOmTRMAdLaibyghISElflspOudhv/76qwgICBBqtVrY2dmJF154QUyZMkX7eknfloUQIi4uTrRp00ZUrlxZqNVq0bNnT3Hq1KkSyzt37pwICQkRarVaODo6isGDB4ucnJwnvl/t2rUT9erVE8eOHRNt27YVtra2ws/PT2zevFkIIUR8fLxo0aKFsLGxES+88IKIjY3VOf/y5cvinXfeES+88IKwsbERzs7Oon///uLSpUvaY1auXFnsfcRD33qLfhc7d+4UTZs2FSqVSsyfP1/7WkhIiBDiwTfB9u3bi6pVq4q0tDTt9fPy8kT9+vVFzZo1xd27d0usZ2kxFLlw4YLo37+/qFKlirC1tRUtW7YUO3bs0LlG0bf1DRs2iA8++EB4enoKSZLE7du3S31/AYiwsDDRvn17MWDAAJ3XRo0aJRo0aKD9HT7c0nq43g9r166daNeunfbnR/92QkJCHltPAGLatGk61/zrr7/EkCFDhIeHh7C2thY+Pj7i7bff1rYMS2ql7N27V/Tv3194eXkJa2trUb16dREeHi7u3bunc+0bN26IwYMHi+eee05YW1sLd3d30bNnT52/j8OHD4ugoCDh4uIibGxshI+PjwgNDS31PRXiQQ+Is7Oz8Pf3L1PLoCLU4eH3vqTfU9HvtaTPECGEWLt2rWjevLm21+nll18Wv/zyi/b1mJgY0bVrV+3vsWbNmiIyMlLn/WnXrl2pn2lyfg5VZHp/fdi+fTtq1qyJ1q1bl+n4YcOGYfXq1ejfvz/effddHDx4EFFRUTh9+jS2bdumc+z58+fRv39/DB06FCEhIVixYgUGDx6Mpk2bol69eujbty+cnJwwfvx4vP766+jatSvs7e31iv/kyZPo3r07GjZsiMjISKhUKpw/fx5//PHHY8/77bff0KVLF9SsWRPTp0/HP//8g0WLFiEgIABJSUnw8fHROX7AgAHw9fVFVFQUkpKS8NVXX8HV1RVz5sx5Yoy3b99G9+7dMXDgQLz66quIjo7GwIEDsW7dOoSHh+Ptt9/GG2+8gU8//RT9+/dHSkoKHBwcADy4kbx//34MHDgQ1atXx+XLlxEdHY327dvj1KlTqFy5Mtq2bYuxY8di4cKF+L//+z/UqVMHALT/BYDk5GS8/vrrGDlyJIYPH47atWsXi1OSJKxYsQINGzbE22+/je+++w4AMG3aNJw8eRLx8fGws7MrsY5t27bF2rVr8eabb6Jjx4546623tK+lpaWhdevWuHfvHsaOHQsXFxesXr0aPXv2xJYtW9CnTx+da82cORPW1taYOHEi8vLyYG1t/cT3+I033sC4ceNw9+5d2Nvb4/79+9i8eTMmTJiA3NzcJ55fViNHjsT169cRGxuLtWvXPvH469evo0WLFsjMzMSIESPg7++Pa9euYcuWLbh3716pddu8eTPu3buHd955By4uLjh06BAWLVqEv/76C5s3b9Ye169fP5w8eRJjxoyBj48P0tPTERsbi6tXr2p/DgoKQrVq1TB58mQ4OTnh8uXL2t9tafbt24eMjAyEh4fD0tJSvzepAtRh5MiReO655zB79myMHTsWzZs3h5ubW6nHz5gxA9OnT0fr1q0RGRkJa2trHDx4ELt27UJQUBAAYNWqVbC3t8eECRNgb2+PXbt2YerUqcjOzsann34K4EELNCsrC3/99Rfmz58PAI/9TDPl51CFpU+Gy8rKEgBEr169ynT80aNHBQAxbNgwnf0TJ04UAMSuXbu0+7y9vQUAsXfvXu2+9PR0oVKpxLvvvqvdV/Tt49NPP9W5ZllbWvPnz3/i/YqSvuE0btxYuLq6ir///lu779ixY8LCwkK89dZbxcobMmSIzjX79OkjXFxcSi2zSNE3r/Xr12v3nTlzRgAQFhYW4sCBA9r9v/zyS7E4H/1WKoQQCQkJAoBYs2aNdt/j7mkV/S527txZ4muPtjiWLl0qAIhvvvlGHDhwQFhaWorw8PAn1lWIf1s+DwsPDxcAxO+//67dd+fOHeHr6yt8fHxEYWGhEOLfb+s1a9Yssd6PKy8jI0NYW1uLtWvXCiGE+PHHH4UkSeLy5ctGbWkJ8fh7WnikpfXWW28JCwsLcfjw4WLHajQanXo//Lsrqf5RUVFCkiRx5coVIYQQt2/fLvHfzsO2bdsmAJRY/uN8/vnnAoDYtm1bmY6vCHV49L0viqmoV6PIo58h586dExYWFqJPnz7av8UiRb+j0uozcuRIUblyZZ2eptLuacn5OVSR6TV6MDs7GwC03+qf5KeffgIATJgwQWf/u+++CwDF7n3VrVsXL7/8svbnatWqoXbt2rh48aI+YT5WUX/1999/D41GU6Zzbty4gaNHj2Lw4MFwdnbW7m/YsCE6duyorefD3n77bZ2fX375Zfz999/a9/Bx7O3tMXDgQO3PtWvXhpOTE+rUqYOWLVtq9xf9/8Pvj62trfb/CwoK8Pfff6NWrVpwcnJCUlJSGWr7gK+vLzp16lSmY0eMGIFOnTphzJgxePPNN+Hn54fZs2eXuaxH/fTTT2jRogXatGmj3Wdvb48RI0bg8uXLOHXqlM7xISEhOvUuiypVqqBz587YsGEDgAcjuFq3bm2S0WKl0Wg0iImJQY8ePUocEfu4YdcP1z8nJwe3bt1C69atIYTAn3/+qT3G2toa8fHxuH37donXKfr3sWPHDhQUFJQ5dn0/G0oidx3KKiYmBhqNBlOnTi127/Th39HD9blz5w5u3bqFl19+Gffu3cOZM2f0LtfUn0MVlV5Jy9HREcCDX0BZXLlyBRYWFqhVq5bOfnd3dzg5OeHKlSs6+2vUqFHsGlWqVCn1j/NpvPbaawgICMCwYcPg5uaGgQMHYtOmTY9NYEVxltRFVqdOHdy6dQs5OTk6+x+tS5UqVQCgTHWpXr16sQ8otVoNLy+vYvseveY///yDqVOnwsvLCyqVClWrVkW1atWQmZmJrKysJ5ZdxNfXt8zHAsDXX3+Ne/fu4dy5c1i1apXeSeRhV65cKfW9LnrdkFiLvPHGG9pupZiYGLzxxhtPdR1juXnzJrKzs1G/fn29z7169ar2w8ze3h7VqlVDu3btAED7e1epVJgzZw5+/vlnuLm5oW3btvjkk0+QmpqqvU67du3Qr18/zJgxA1WrVkWvXr2wcuVK5OXlPbZ8fT8bKmIdyurChQuwsLBA3bp1H3vcyZMn0adPH6jVajg6OqJatWoYNGiQTn30UfR3X6NGDWRnZ+tsNWvWxK1bt3Djxg1kZ2dr61qlShWd44oGdly9etWo3eCmpHfS8vT0xIkTJ/QqpKwP5pXWFy6EeOoyCgsLdX62tbXF3r178dtvv+HNN9/Ef//7X7z22mvo2LFjsWMNYUhdSju3LNccM2YMZs2ahQEDBmDTpk349ddfERsbCxcXlzK3LAHonXTi4+O1/1COHz+u17mGetoE2bNnT6hUKoSEhCAvLw8DBgwo9diy/n3JobCwEB07dsSPP/6ISZMmISYmBrGxsdqHUh/+vYeHh+Ps2bOIioqCjY0NIiIiUKdOHW1LRpIkbNmyBQkJCRg9ejSuXbuGIUOGoGnTprh7926pMfj7+wN4+t99RaiDMWVmZqJdu3Y4duwYIiMjsX37dsTGxmrvJenzb7FIXl4e3F0tMXr0aKjVap0tOjoaAODp6Qm1Wo2PP/4YAPD888/rHDdq1CgAQIMGDeDr66vIxKX3w8Xdu3fHhQsXkJCQ8MRjvb29odFocO7cOZ39aWlpyMzMNGpXTJUqVZCZmVls/6PfygHAwsICr7zyCubNm4dTp05h1qxZ2LVrF3bv3l3itYviTE5OLvbamTNnULVq1VIHHJjali1bEBISgrlz56J///7o2LEj2rRpU+y9MeYT/jdu3MCYMWMQFBSE7t27Y+LEiSW+72Xl7e1d6ntd9Lox2Nraonfv3oiPj0fHjh1RtWrVUo/V5+/rUWV9r6tVqwZHR0e9vxQeP34cZ8+exdy5czFp0iT06tULgYGB8PT0LPF4Pz8/vPvuu/j1119x4sQJ5OfnY+7cuTrHvPTSS5g1axaOHDmCdevW4eTJk9i4cWOpMbRp0wZVqlTBhg0bniqRV4Q6lJWfnx80Gk2xbuqHxcfH4++//8aqVaswbtw4dO/eHYGBgdoel4eV9e/Dw8MDqemFuJTojb/P+hq0XUr0RmpqKvLz85/6fZCL3knr/fffh52dHYYNG4a0tLRir1+4cAGff/45AKBr164AUOxp73nz5gEAunXrpm/xpfLz80NWVhb++9//avfduHGj2AjFjIyMYuc2btwYAErtPvDw8EDjxo2xevVqnQ+uEydO4Ndff9XWsyKwtLQs1ppbtGhRsQ+SoiRb0gexvoYPHw6NRoOvv/4ay5Ytg5WVFYYOHVqmVmVJunbtikOHDul8McrJycGyZcvg4+PzxG4ZfUycOBHTpk1DRETEY4/z8/PDgQMHdP6R79ixAykpKU8so6zvtYWFBXr37o3t27fjyJEjxV4v7f0saoE//LoQQvvvsMi9e/eKfbP28/ODg4OD9m//9u3bxcp50r8PAKhcuTImTZqE06dPY9KkSSXG+s033+DQoUMVtg5l1bt3b1hYWCAyMrJYi6mo3JLqk5+fjy+//LLY9ezs7MrUXeju7v7geHvjbEql95B3Pz8/rF+/Hq+99hrq1KmDt956C/Xr10d+fj7279+PzZs3a+ema9SoEUJCQrBs2TJtc/nQoUNYvXo1evfujQ4dOhitIgMHDsSkSZPQp08fjB07Fvfu3UN0dDReeOEFnQEIkZGR2Lt3L7p16wZvb2+kp6fjyy+/RPXq1XVu/D/q008/RZcuXdCqVSsMHTpUO9RUrVZXqCl6unfvjrVr10KtVqNu3bpISEjAb7/9BhcXF53jGjduDEtLS8yZMwdZWVlQqVT4z3/+A1dXV73KW7lyJX788UesWrUK1atXB/AgSQ4aNAjR0dHa7gh9TJ48GRs2bECXLl0wduxYODs7Y/Xq1bh06RK2bt362AeH9dWoUSM0atToiccNGzYMW7ZsQefOnTFgwABcuHAB33zzDfz8/J54btOmTQEAY8eORadOnWBpaakz0OZhs2fPxq+//op27dphxIgRqFOnDm7cuIHNmzdj3759JT6I7e/vDz8/P0ycOBHXrl2Do6Mjtm7dWuz+6dmzZ/HKK69gwIABqFu3LqysrLBt2zakpaVp41m9ejW+/PJL9OnTB35+frhz5w6WL18OR0fHJ345e++993Dy5EnMnTsXu3fvRv/+/eHu7o7U1FTExMTg0KFD2L9/f4nnVpQ6lEWtWrXwwQcfYObMmXj55ZfRt29fqFQqHD58GJ6enoiKikLr1q1RpUoVhISEYOzYsZAkCWvXri0xmTdt2hTffvstJkyYgObNm8Pe3h49evQwOM5n1tMOOzx79qwYPny48PHxEdbW1sLBwUEEBASIRYsW6QznLCgoEDNmzBC+vr6iUqVKwsvL67EPFz+qtCHFJQ15/fXXX0X9+vWFtbW1qF27tvjmm2+KDVeNi4sTvXr1Ep6ensLa2lp4enqK119/XZw9e7ZYGY8+1Pfbb7+JgIAAYWtrKxwdHUWPHj1Kfajv0SH1RQ/TPvwAZEmKHi5+VGnvDx4ZMn779m0RGhoqqlatKuzt7UWnTp3EmTNnShyyvXz5clGzZk1haWlZ4sPFJXn4OikpKUKtVosePXoUO65Pnz7Czs5OXLx48bH1fTT+IkUPFzs5OQkbGxvRokWLUh8ufnSI8tOU97DSfodz584Vzz33nFCpVCIgIEAcOXKkTEPe79+/L8aMGSOqVasmJEl64sPFV65cEW+99ZaoVq2aUKlUombNmiIsLOyxDxefOnVKBAYGCnt7e1G1alUxfPhwcezYMZ1Ybt26JcLCwoS/v7+ws7MTarVatGzZUmzatEl7naSkJPH666+LGjVqCJVKJVxdXUX37t3FkSNHyvDuPrBlyxYRFBQknJ2dhZWVlfDw8BCvvfaaiI+P1x5TEerw6Htf1iHvRVasWCGaNGkiVCqVqFKlimjXrp3Ow/5//PGHeOmll4Stra3w9PQU77//vvYxlYfrfffuXfHGG28IJyenxz5cXPTIUWpyDXHvuo9BW2pyDQFAZGVlleVXWqFIQjxlHw4REZlMdnY21Go1ridXh6ODYb0N2Xc08Kz9F7KysrQjP5VCkUuTEBGReao4s1cSEdETFQqBQgM7yAw9X05MWkRECqKBgAaGJR1Dz5cTuweJiEgx2NIiIlIQDQQKzbilxaRFRKQg7B4kIiJSCLa0iIgUxNxHD5ptS2vx4sXw8fGBjY0NWrZsWeqcaEoWFRWF5s2bw8HBAa6urujdu3eJE9E+iz7++GNIkoTw8HC5Qyk3165dw6BBg+Di4gJbW1s0aNCgxDkLnwWFhYWIiIiAr68vbG1t4efnh5kzZz71/JZKpjHSplRmmbSK5vmaNm0akpKS0KhRI3Tq1Anp6elyh2ZUe/bsQVhYGA4cOIDY2FgUFBQgKCio2Npfz5rDhw9j6dKlaNiwodyhlJvbt28jICAAlSpVws8//4xTp05h7ty5Jc4i/iyYM2cOoqOj8cUXX+D06dOYM2cOPvnkEyxatEju0Eyu8H8DMQzdlMosk9a8efMwfPhwhIaGom7duliyZAkqV66MFStWyB2aUe3cuRODBw9GvXr10KhRI6xatQpXr15FYmKi3KGVm7t37yI4OBjLly9/Zj/AgQcf4l5eXli5ciVatGgBX19fBAUFlWkCXyXav38/evXqhW7dusHHxwf9+/dHUFDQM9lDUhHt3bsXPXr0gKenJyRJQkxMjPa1goICTJo0CQ0aNICdnR08PT3x1ltv4fr16zrXyMjIQHBwMBwdHeHk5IShQ4c+1fpmZpe08vPzkZiYiMDAQO0+CwsLBAYGlmmNMCUrWv7g4aW6nzVhYWHo1q2bzu/3WfTDDz+gWbNmePXVV+Hq6oomTZpg+fLlcodVblq3bo24uDicPXsWAHDs2DHs27cPXbp0kTky0ysUxtn0kZOTg0aNGmHx4sXFXrt37x6SkpIQERGBpKQkfPfdd0hOTkbPnj11jgsODsbJkycRGxuLHTt2YO/evRgxYoTe9Te7gRi3bt1CYWEh3NzcdPa7ublpFxl8Fmk0GoSHhyMgIOCplnNXgo0bNyIpKQmHDx+WO5Ryd/HiRURHR2PChAn4v//7Pxw+fBhjx46FtbU1QkJC5A7P6CZPnozs7Gz4+/vD0tIShYWFmDVrFoKDg+UOzeSMcU9K3/O7dOlS6hcEtVqN2NhYnX1ffPEFWrRogatXr6JGjRo4ffo0du7cicOHD6NZs2YAHixh1LVrV3z22WelLvZZErNLWuYqLCwMJ06cwL59++QOpVykpKRg3LhxiI2NhY2NjdzhlDuNRoNmzZph9uzZAIAmTZrgxIkTWLJkyTOZtDZt2oR169Zh/fr1qFevHo4ePYrw8HB4eno+k/U1lezsbJ2fVSoVVCqVwdfNysqCJEna9d8SEhLg5OSkTVgAEBgYCAsLCxw8eBB9+vQp87XNrnuwatWqsLS0LLbqclpamnZl0GfN6NGjsWPHDuzevVu7UOOzJjExEenp6XjxxRdhZWUFKysr7NmzBwsXLoSVldVTLQFfkXl4eBRbwblOnTq4evWqTBGVr/feew+TJ0/GwIED0aBBA7z55psYP348oqKi5A7N5DSQUGjgpoEEAPDy8oJardZuxng/c3NzMWnSJLz++uvaZU9SU1OLLTBrZWUFZ2dnpKam6nV9s2tpWVtbo2nTpoiLi0Pv3r0BPPjWGhcXh9GjR8sbnJEJITBmzBhs27YN8fHx8PX1lTukcvPKK6/g+PHjOvtCQ0Ph7++PSZMmaZc/f1YEBAQUe3zh7Nmz8Pb2limi8nXv3r1iK1ZbWloWW+7eHGjEg83QawAPeigeXk/L0FZWQUEBBgwYACEEoqOjDbpWacwuaQHAhAkTEBISgmbNmqFFixZYsGABcnJyEBoaKndoRhUWFob169fj+++/h4ODg/YbjVqthq2trczRGZeDg0Oxe3V2dnZwcXF5Ju/hjR8/Hq1bt8bs2bMxYMAAHDp0CMuWLcOyZcvkDq1c9OjRA7NmzUKNGjVQr149/Pnnn5g3bx6GDBkid2iK5ujoaLRFIIsS1pUrV7Br1y6d67q7uxd7pOj+/fvIyMjQu4fLLJPWa6+9hps3b2Lq1KlITU1F48aNsXPnzmKDM5Su6JtO+/btdfavXLkSgwcPNn1AZDTNmzfHtm3bMGXKFERGRsLX1xcLFix4ZgcmLFq0CBERERg1ahTS09Ph6emJkSNHYurUqXKHZnJFXXyGXsOYihLWuXPnsHv3bri4uOi83qpVK2RmZiIxMRFNmzYFAOzatQsajQYtW7bUqyxJmOMj5URECpOdnQ21Wo39Jz1g72DYcIS7dzRoXe8GsrKyytTSunv3Ls6fPw/gwaCfefPmoUOHDnB2doaHhwf69++PpKQk7NixQ+fLv7OzM6ytrQE8GIGYlpaGJUuWoKCgAKGhoWjWrBnWr1+vV+xMWkRECiBn0oqPj0eHDh2K7Q8JCcH06dNLvV++e/dubU9PRkYGRo8eje3bt8PCwgL9+vXDwoULYW9vr1fsTFpERApQlLT2nfA0StJqU/96mZNWRWKW97SIiJSqIt7TMiWze06LiIiUiy0tIiIFKYQFCg1sbyj5UXsmLSIiBRFCgkYY1r0nDDxfTkxaREQKwntaZiovLw/Tp09HXl6e3KGYhDnV15zqCphXfc2prlQysx3yXjR8VIlDPp+GOdXXnOoKmFd9zamujyqq+8//9YWdgUPec+5o0KXhJUW+j+weJCJSEA0kaAzsJNNAuW0Vs+0eJCIi5VF0S0uj0eD69etwcHCAJOl3Y7Fo8bNHF0F7VplTfc2proB51VeJdRVC4M6dO/D09Cy2vMrTMPeBGIpOWtevX4eXl5dB1zD0fKUxp/qaU10B86qvEuuakpJilEVYC4UFCoWBz2kpeCiDopOWg4MDACDugCvs7E3b0zm5vn7T6SuZhY3hy28/Vbn/W6rbXNz3qipPwYlnTF6klYfrkw8qB+LePyYv877Ix57MDdrPKzKMopNWUZegnb2FwRNI6stKqmTS8uRkIVnLU66FPOXKxspGnnJl+Fu2spDni5CQ5JsLQt9bGKV5MBDDsGsZer6cFJ20iIjMjcYI0zhx9CAREZEJsKVFRKQgHIhBRESKoYEFHy4mIiJSAra0iIgUpFBIKDRwaRFDz5cTkxYRkYIYZxFIdg8aZPHixfDx8YGNjQ1atmyJQ4cOyR0SEVGFpBEWRtmUSvbIv/32W0yYMAHTpk1DUlISGjVqhE6dOiE9PV3u0IiIqIKRPWnNmzcPw4cPR2hoKOrWrYslS5agcuXKWLFihdyhERFVOEXdg4ZuSiVr5Pn5+UhMTERgYKB2n4WFBQIDA5GQkCBjZEREFZMG/w7GeNpNI3clDCDrQIxbt26hsLAQbm5uOvvd3Nxw5kzxSTzz8vJ0ltlW0vIERERkOEW1EaOioqBWq7WbEpcnICIyRNHDxYZuSiVr5FWrVoWlpSXS0tJ09qelpcHd3b3Y8VOmTEFWVpZ2S0lJMVWoREQVQtE0ToZuSiVr5NbW1mjatCni4uK0+zQaDeLi4tCqVatix6tUKjg6OupsRERkPmR/uHjChAkICQlBs2bN0KJFCyxYsAA5OTkIDQ2VOzQiogqH62nJ7LXXXsPNmzcxdepUpKamonHjxti5c2exwRlERGSsWd6V2z0oe9ICgNGjR2P06NFyh0FERBVchUhaRERUNsaZe5AtLSIiMgGNkKAxcJZ2Q8+Xk3LTLRERmR22tIiIFERjhO5BJT9czKRFRKQgxlhaRMlLkzBpEREpSCEkFBr4nJWh58tJuemWiIjMDltaREQKwu7BZ8AHL3eClWRt2kLj7Exb3v9YhQiTlykKCkxeJgDk+xWfNNkUrI4UXxbHFCxz8558UDmQ3KqZvExNVbXJywTk6VqSNJbAbeNdrxCGd+8VGicUWSg33RIRkdl5JlpaRETmgt2DRESkGOY+Ya5yIyciIrPDlhYRkYIII6ynJRT8nBaTFhGRgrB7kIiISCGYtIiIFKRoaRJDN33s3bsXPXr0gKenJyRJQkxMjM7rQghMnToVHh4esLW1RWBgIM6dO6dzTEZGBoKDg+Ho6AgnJycMHToUd+/e1bv+TFpERApStAikoZs+cnJy0KhRIyxevLjE1z/55BMsXLgQS5YswcGDB2FnZ4dOnTohNzdXe0xwcDBOnjyJ2NhY7NixA3v37sWIESP0rj/vaRER0WN16dIFXbp0KfE1IQQWLFiADz/8EL169QIArFmzBm5uboiJicHAgQNx+vRp7Ny5E4cPH0azZs0AAIsWLULXrl3x2WefwdPTs8yxsKVFRKQgcnQPPs6lS5eQmpqKwMBA7T61Wo2WLVsiISEBAJCQkAAnJydtwgKAwMBAWFhY4ODBg3qVx5YWEZGCaGBh8CKORednZ2fr7FepVFCpVHpdKzU1FQDg5uams9/NzU37WmpqKlxdXXVet7KygrOzs/aYsmJLi4hIQQqFZJQNALy8vKBWq7VbVFSUzLV7Mra0iIjMVEpKChwdHbU/69vKAgB39werMaSlpcHDw0O7Py0tDY0bN9Yek56ernPe/fv3kZGRoT2/rNjSIiJSEGPe03J0dNTZniZp+fr6wt3dHXFxcdp92dnZOHjwIFq1agUAaNWqFTIzM5GYmKg9ZteuXdBoNGjZsqVe5bGlRUSkIMIIs7wLPc+/e/cuzp8/r/350qVLOHr0KJydnVGjRg2Eh4fjo48+wvPPPw9fX19ERETA09MTvXv3BgDUqVMHnTt3xvDhw7FkyRIUFBRg9OjRGDhwoF4jBwEmLSIieoIjR46gQ4cO2p8nTJgAAAgJCcGqVavw/vvvIycnByNGjEBmZibatGmDnTt3wsbGRnvOunXrMHr0aLzyyiuwsLBAv379sHDhQr1jYdIiIlKQQkhGWLlYv/Pbt28PIUpfNV2SJERGRiIyMrLUY5ydnbF+/Xq9yi0JkxYRkYJoBAx+zkpTev6p8DgQg4iIFIMtLSIiBdEYYSCGoefLiUmLiEhBNEZYBNLQ8+Wk3HRLRERmhy0tIiIFeXgaJkOuoVRMWkRECmLu97SUGzkREZkdtrSIiBREA8PXw1LyQAwmLSIiBRFGGD0omLSIiMgUjLHysDFXLja1ZyNpFRYCUqFpy/zA2bTl/c/ZT0x/G7La9zZPPqgcOGw+LEu5Uu2aspQrLqXIUi4KTfxvB4B0Q2PyMgFAjtmLhOa+DKU+u56NpEVEZCbMffQgkxYRkYKYe/egctMtERGZHba0iIgUxNznHmTSIiJSEHYPEhERKQRbWkRECmLuLS0mLSIiBTH3pMXuQSIiUgy2tIiIFIQtLRlFRUWhefPmcHBwgKurK3r37o3k5GQ5QyIiqtAE/h32/rSbHNNZGYusSWvPnj0ICwvDgQMHEBsbi4KCAgQFBSEnJ0fOsIiIKqyilpahm1LJ2j24c+dOnZ9XrVoFV1dXJCYmom3btjJFRUREFVWFuqeVlZUFAHB2LnkG9by8POTl5Wl/zs7ONklcREQVBe9pVRAajQbh4eEICAhA/fr1SzwmKioKarVau3l5eZk4SiIieZl792CFSVphYWE4ceIENm7cWOoxU6ZMQVZWlnZLSZFp/SEiIpJFhegeHD16NHbs2IG9e/eievXqpR6nUqmgUqlMGBkRUcVi7t2DsiYtIQTGjBmDbdu2IT4+Hr6+vnKGQ0RU4QkhQRiYdAw9X06yJq2wsDCsX78e33//PRwcHJCamgoAUKvVsLW1lTM0IiKqgGS9pxUdHY2srCy0b98eHh4e2u3bb7+VMywiogrL0AeLjbEel5xk7x4kIqKyM/d7WhVm9CAREdGTVIjRg0REVDYciEFERIrB7kEiIiKFYEuLiEhB2D1IRESKIYzQPcikRUREJiEAGPq0kJIfNnomkpYoLISQCk1apmWyPJP1ei83/VRXf4dnmrxMAFB/X0mWcqV7ubKUC5lmgRFebiYvU7p83eRlAkBBEz+Tl3n/fi6QbvJin1nPRNIiIjIXGkiQDJzRgjNiEBGRSZj7QAwOeSciIsVgS4uISEE0QoJkxg8XM2kRESmIEEYYPajg4YPsHiQiIsVgS4uISEE4EIOIiBSjKGkZuumjsLAQERER8PX1ha2tLfz8/DBz5kydNRGFEJg6dSo8PDxga2uLwMBAnDt3ztjVZ9IiIqLHmzNnDqKjo/HFF1/g9OnTmDNnDj755BMsWrRIe8wnn3yChQsXYsmSJTh48CDs7OzQqVMn5OYa92F9dg8SESmIHKMH9+/fj169eqFbt24AAB8fH2zYsAGHDh0C8KCVtWDBAnz44Yfo1asXAGDNmjVwc3NDTEwMBg4caFC8D2NLi4hIQYpGDxq6AUB2drbOlpeXV2KZrVu3RlxcHM6ePQsAOHbsGPbt24cuXboAAC5duoTU1FQEBgZqz1Gr1WjZsiUSEhKMWn+2tIiIzJSXl5fOz9OmTcP06dOLHTd58mRkZ2fD398flpaWKCwsxKxZsxAcHAwASE1NBQC4uenOY+nm5qZ9zViYtIiIFORBS8nQ0YMP/puSkgJHR0ftfpVKVeLxmzZtwrp167B+/XrUq1cPR48eRXh4ODw9PRESEmJQLPpi0iIiUhBjDnl3dHTUSVqlee+99zB58mTtvakGDRrgypUriIqKQkhICNzd3QEAaWlp8PDw0J6XlpaGxo0bGxTro3hPi4hIQYSRNn3cu3cPFha66cLS0hIajQYA4OvrC3d3d8TFxWlfz87OxsGDB9GqVSs9S3s8trSIiOixevTogVmzZqFGjRqoV68e/vzzT8ybNw9DhgwBAEiShPDwcHz00Ud4/vnn4evri4iICHh6eqJ3795GjYVJi4hIQeSYEWPRokWIiIjAqFGjkJ6eDk9PT4wcORJTp07VHvP+++8jJycHI0aMQGZmJtq0aYOdO3fCxsbGoFgfxaRFRKQkT9O/V9I19ODg4IAFCxZgwYIFpR4jSRIiIyMRGRlpWGxPwHtaRESkGGxpEREpiRG6B6HgCXOZtIiIFITraRERESkEW1pERApi7utpMWkRESmJkAy/J6XgpMXuQSIiUgy2tIiIFMTcB2IwaRERKYkMDxdXJOweJCIixShTS+uHH34o8wV79uz51MEQEdHjcfRgGZR1ll5JklBYWGhIPE9FsrGBZGFt2kLdqpq2vP+x2nfC5GW67s43eZkA8PP1o7KU26XmS7KUa1HFSZZypZuZJi/z/t0ck5cJAJVOXjV5mZKmHP79KLh7z1BlSlpFa6YQEZG8zL2lZdA9rdzcXGPFQURE9ER6J63CwkLMnDkTzz33HOzt7XHx4kUAQEREBL7++mujB0hERA+RY+niCkTvpDVr1iysWrUKn3zyCayt/72PVL9+fXz11VdGDY6IiB4lGWlTJr2T1po1a7Bs2TIEBwfD0tJSu79Ro0Y4c+aMUYMjIiJ6mN4PF1+7dg21atUqtl+j0aCgoMAoQRERUSn4cLF+6tati99//73Y/i1btqBJkyZGCYqIiEph5ve09G5pTZ06FSEhIbh27Ro0Gg2+++47JCcnY82aNdixY0d5xEhERATgKVpavXr1wvbt2/Hbb7/Bzs4OU6dOxenTp7F9+3Z07NixPGIkIqIiRUuTGLop1FNNmPvyyy8jNjbW2LEQEdETmPss70/9cPGRI0ewdu1arF27FomJiQYH8vHHH0OSJISHhxt8LSIiejbp3dL666+/8Prrr+OPP/6Ak5MTACAzMxOtW7fGxo0bUb16db2DOHz4MJYuXYqGDRvqfS4RkVnh6EH9DBs2DAUFBTh9+jQyMjKQkZGB06dPQ6PRYNiwYXoHcPfuXQQHB2P58uWoUqWK3ucTEZkVM7+npXfS2rNnD6Kjo1G7dm3tvtq1a2PRokXYu3ev3gGEhYWhW7duCAwM1PtcIiIyL3p3D3p5eZX4EHFhYSE8PT31utbGjRuRlJSEw4cPl+n4vLw85OXlaX/Ozs7WqzwiIqWTxIPN0Gsold4trU8//RRjxozBkSNHtPuOHDmCcePG4bPPPivzdVJSUjBu3DisW7cONjY2ZTonKioKarVau3l5eekbPhGRsvHh4ierUqUKJOnfPtCcnBy0bNkSVlYPTr9//z6srKwwZMiQMi8YmZiYiPT0dLz44ovafYWFhdi7dy+++OIL5OXl6cxtCABTpkzBhAkTtD9nZ2czcRGReTHGPSkF39MqU9JasGCB0Qt+5ZVXcPz4cZ19oaGh8Pf3x6RJk4olLABQqVRQqVRGj4WIiJShTEkrJCTE6AU7ODigfv36Ovvs7Ozg4uJSbD8REf2PmQ95f6oZMYrk5uYiPz9fZ5+jo6NBARER0WMwaeknJycHkyZNwqZNm/D3338Xe72wsPCpg4mPj3/qc4mI6Nmn9+jB999/H7t27UJ0dDRUKhW++uorzJgxA56enlizZk15xEhEREU4elA/27dvx5o1a9C+fXuEhobi5ZdfRq1ateDt7Y1169YhODi4POIkIiLA7EcP6t3SysjIQM2aNQE8uH+VkZEBAGjTps1TzYhBRERUVnonrZo1a+LSpUsAAH9/f2zatAnAgxZY0QS6RERUPopmxDB0Uyq9k1ZoaCiOHTsGAJg8eTIWL14MGxsbjB8/Hu+9957RAyQioofwnpZ+xo8fr/3/wMBAnDlzBomJiahVqxaXFiEionJl0HNaAODt7Q1vb29jxEJERPRYZUpaCxcuLPMFx44d+9TBEBHR40kwwizvRolEHmVKWvPnzy/TxSRJkiVpSZUqQbKoZNIyRdotk5ZXxMLZyeRl3vfzMHmZANC1iass5Wp+tJalXKl/jjzl2lU2eZmWNWuYvEwAkHLzn3yQscvUKDlFVDxlSlpFowWJiEhmZv6clsH3tIiIyIQ49yARESmGmSctvZ/TIiIikgtbWkRECmKMGS2UPCMGkxYRkZKwe1B/v//+OwYNGoRWrVrh2rVrAIC1a9di3759Rg2OiIgqhmvXrmHQoEFwcXGBra0tGjRogCNHjmhfF0Jg6tSp8PDwgK2tLQIDA3Hu3Dmjx6F30tq6dSs6deoEW1tb/Pnnn8jLywMAZGVlYfbs2UYPkIiIHiLD3IO3b99GQEAAKlWqhJ9//hmnTp3C3LlzUaVKFe0xn3zyCRYuXIglS5bg4MGDsLOzQ6dOnZCbm2tYfR+hd9L66KOPsGTJEixfvhyVKv37QG9AQACSkpKMGhwREemSY5b3OXPmwMvLCytXrkSLFi3g6+uLoKAg+Pn5AXjQylqwYAE+/PBD9OrVCw0bNsSaNWtw/fp1xMTEGLX+eiet5ORktG3btth+tVqNzMxMY8REREQmkJ2drbMV9Zw96ocffkCzZs3w6quvwtXVFU2aNMHy5cu1r1+6dAmpqakIDAzU7lOr1WjZsiUSEhKMGrPeScvd3R3nz58vtn/fvn3axSGJiKicFM2IYegGwMvLC2q1WrtFRUWVWOTFixcRHR2N559/Hr/88gveeecdjB07FqtXrwYApKamAgDc3Nx0znNzc9O+Zix6jx4cPnw4xo0bhxUrVkCSJFy/fh0JCQmYOHEiIiIijBocERE9woijB1NSUuDo6KjdrVKpSjxco9GgWbNm2nELTZo0wYkTJ7BkyRKEhIQYGIx+9E5akydPhkajwSuvvIJ79+6hbdu2UKlUmDhxIsaMGVMeMRIRUTlwdHTUSVql8fDwQN26dXX21alTB1u3bgXwoAcOANLS0uDh8e8E22lpaWjcuLHxAsZTdA9KkoQPPvgAGRkZOHHiBA4cOICbN29i5syZRg2MiIiKk2MgRkBAAJKTk3X2nT17VruWoq+vL9zd3REXF6d9PTs7GwcPHkSrVq0MrvPDnvrhYmtr62KZl4iIypkMDxePHz8erVu3xuzZszFgwAAcOnQIy5Ytw7JlywA8aMyEh4fjo48+wvPPPw9fX19ERETA09MTvXv3NjBYXXonrQ4dOkCSSp/WfteuXQYFREREj2GEaZz0TVrNmzfHtm3bMGXKFERGRsLX1xcLFixAcHCw9pj3338fOTk5GDFiBDIzM9GmTRvs3LkTNjY2BgarS++k9Wj/ZEFBAY4ePYoTJ06Y/IYcERGZRvfu3dG9e/dSX5ckCZGRkYiMjCzXOPROWqWtYjx9+nTcvXvX4ICIiOgxOPegcQwaNAgrVqww1uWIiKgkMkzjVJEYLWklJCQYve+SiIjoYXp3D/bt21fnZyEEbty4gSNHjvDhYiKicsb1tPSkVqt1frawsEDt2rURGRmJoKAgowVGRET0KL2SVmFhIUJDQ9GgQQOdKemJiIhMQa97WpaWlggKCuJs7kREcuFADP3Ur18fFy9eLI9YiIjoCeSYxqkieapFICdOnIgdO3bgxo0bxdZjISIiKi9lvqcVGRmJd999F127dgUA9OzZU2c6JyEEJElCYWGh8aMkIqJ/KbilZKgyJ60ZM2bg7bffxu7du8szHiIiehwznxGjzElLiAe1bNeuXbkFQ0RE9Dh6DXl/3OzusrK2AiwqmbRI2d4Jm5JXFi1PlknJTz6oHEhVnGQpFx1vyFLslqsJspTbp4Zx1zsqC0t7O5OXCcjTwBAi36jX48PFenjhhReemLgyMjIMCoiIiB6D3YNlN2PGjGIzYhARkemwpaWHgQMHwtXVtbxiISIieqwyJ60Kez+LiMicsHuwbIpGDxIRkYyYtMpGo9GUZxxERERPpPfSJEREJB8OxCAiIuUw8+5BvSfMJSIikgtbWkRESsKWlryuXbuGQYMGwcXFBba2tmjQoAGOHDkid1hERBWSua+nJWtL6/bt2wgICECHDh3w888/o1q1ajh37hyqVKkiZ1hERFRByZq05syZAy8vL6xcuVK7z9fXV8aIiIgqOHYPyueHH35As2bN8Oqrr8LV1RVNmjTB8uXLSz0+Ly+PKyUTkVkz9+5BWZPWxYsXER0djeeffx6//PIL3nnnHYwdOxarV68u8fioqCio1Wrt5uXlZeKIiYhITrImLY1GgxdffBGzZ89GkyZNMGLECAwfPhxLliwp8fgpU6YgKytLu6WkpJg4YiIimQkjbQol6z0tDw8P1K1bV2dfnTp1sHXr1hKPV6lUUKlMvwgiEVGFYeb3tGRNWgEBAUhO1l0V9+zZs/D29pYpIiKiik2C4SunK3nNDlm7B8ePH48DBw5g9uzZOH/+PNavX49ly5YhLCxMzrCIiKiCkjVpNW/eHNu2bcOGDRtQv359zJw5EwsWLEBwcLCcYRERVVy8pyWv7t27o3v37nKHQUSkCOY+y7vs0zgRERGVlewtLSIi0gNHDxIRkaIoOOkYit2DRESkGGxpEREpiLkPxGDSIiJSEjO/p8XuQSIiUgy2tIiIFITdg88AkZkNIVnLHYZJiOw7Ji/Twq2aycsEAFFJnj9PS3s7WcrtU6OVLOXOvpBg8jIjGgWavEwAgKuL6csszAOMufQfuweJiIiU4ZloaRERmQt2DxIRkXKYefcgkxYRkZKYedLiPS0iIiqzjz/+GJIkITw8XLsvNzcXYWFhcHFxgb29Pfr164e0tLRyKZ9Ji4hIQYruaRm6PY3Dhw9j6dKlaNiwoc7+8ePHY/v27di8eTP27NmD69evo2/fvkaobXFMWkRESiLTIpB3795FcHAwli9fjipVqmj3Z2Vl4euvv8a8efPwn//8B02bNsXKlSuxf/9+HDhw4OnrWQomLSIiM5Wdna2z5eXllXpsWFgYunXrhsBA3WfsEhMTUVBQoLPf398fNWrUQEKC8Z8BZNIiIlIQSQijbADg5eUFtVqt3aKiokosc+PGjUhKSirx9dTUVFhbW8PJyUlnv5ubG1JTU41ef44eJCJSEiOOHkxJSYGjo6N2t0qlKnZoSkoKxo0bh9jYWNjY2BhYsOHY0iIiMlOOjo46W0lJKzExEenp6XjxxRdhZWUFKysr7NmzBwsXLoSVlRXc3NyQn5+PzMxMnfPS0tLg7u5u9JjZ0iIiUhBTz4jxyiuv4Pjx4zr7QkND4e/vj0mTJsHLywuVKlVCXFwc+vXrBwBITk7G1atX0aqV8efTZNIiIlISEz9c7ODggPr16+vss7Ozg4uLi3b/0KFDMWHCBDg7O8PR0RFjxoxBq1at8NJLLxkYaHFMWkREZJD58+fDwsIC/fr1Q15eHjp16oQvv/yyXMpi0iIiUpCKMGFufHy8zs82NjZYvHgxFi9ebNiFy4BJi4hISTj3IBERkTKwpUVEpCAVoXtQTkxaRERKYubdg0xaREQKo+SWkqF4T4uIiBSDLS0iIiUR4sFm6DUUikmLiEhBzH0gBrsHiYhIMdjSIiJSEo4eJCIipZA0DzZDr6FU7B4kIiLFYEuLiEhJ2D34DHjOHbAsvuJmecp3tTNpeUUq3c41eZly9SRIf6XJUq7meS9ZyrVKy5Sl3IhGgSYv81po/ScfVA6qb7po8jIlTb5xr8fRg0RERMrwbLS0iIjMBR8uJiIipWD3IBERkUKwpUVEpCQcPUhEREph7t2DTFpEREpi5gMxeE+LiIgUgy0tIiIFYfcgEREph5kPxGD3IBERKYasSauwsBARERHw9fWFra0t/Pz8MHPmTAgF3yQkIipPRd2Dhm5KJWv34Jw5cxAdHY3Vq1ejXr16OHLkCEJDQ6FWqzF27Fg5QyMiqpg04sFm6DUUStaktX//fvTq1QvdunUDAPj4+GDDhg04dOiQnGEREVEFJWv3YOvWrREXF4ezZ88CAI4dO4Z9+/ahS5cuJR6fl5eH7OxsnY2IyKwII20KJWtLa/LkycjOzoa/vz8sLS1RWFiIWbNmITg4uMTjo6KiMGPGDBNHSURUcUgwwpB3o0QiD1lbWps2bcK6deuwfv16JCUlYfXq1fjss8+wevXqEo+fMmUKsrKytFtKSoqJIyYiIjnJ2tJ67733MHnyZAwcOBAA0KBBA1y5cgVRUVEICQkpdrxKpYJKZdoViomIKhQzn8ZJ1qR17949WFjoNvYsLS2h0ci1wDsRUcXGGTFk1KNHD8yaNQs1atRAvXr18Oeff2LevHkYMmSInGEREVVcZj4jhqxJa9GiRYiIiMCoUaOQnp4OT09PjBw5ElOnTpUzLCIiqqBkTVoODg5YsGABFixYIGcYRESKIQkBycB7UoaeLydOmEtEpCSa/22GXkOhOGEuEREpBltaREQKwu5BIiJSDjMfPcjuQSIiUgy2tIiIlIQzYhARkVJwRoxngHQ7G5KFtUnLtL5126TlFZFsTD/3YvaLniYvEwAcrtyQpVzZFBbKUqxURW3yMp/7UZ7fbfIEX5OXqcnNBT40ebHPrGciaRERmQ12DxIRkVJImgeboddQKo4eJCIixWBLi4hIScy8e5AtLSIiJRFG2vQQFRWF5s2bw8HBAa6urujduzeSk5N1jsnNzUVYWBhcXFxgb2+Pfv36IS0t7enrWQomLSIiBSmaxsnQTR979uxBWFgYDhw4gNjYWBQUFCAoKAg5OTnaY8aPH4/t27dj8+bN2LNnD65fv46+ffsau/rsHiQiosfbuXOnzs+rVq2Cq6srEhMT0bZtW2RlZeHrr7/G+vXr8Z///AcAsHLlStSpUwcHDhzASy+9ZLRY2NIiIlKSontahm4AsrOzdba8vLwyhZCVlQUAcHZ2BgAkJiaioKAAgYGB2mP8/f1Ro0YNJCQkGLX6TFpEREoi8O+aWk+7/a930MvLC2q1WrtFRUU9sXiNRoPw8HAEBASgfv36AIDU1FRYW1vDyclJ51g3NzekpqYaWGFd7B4kIjJTKSkpcHR01P6sUj15xp2wsDCcOHEC+/btK8/QSsWkRUSkIMZcT8vR0VEnaT3J6NGjsWPHDuzduxfVq1fX7nd3d0d+fj4yMzN1WltpaWlwd3c3KNZHsXuQiEhJBIxwT0vPIoXA6NGjsW3bNuzatQu+vrpzODZt2hSVKlVCXFycdl9ycjKuXr2KVq1aGaHS/2JLi4iIHissLAzr16/H999/DwcHB+19KrVaDVtbW6jVagwdOhQTJkyAs7MzHB0dMWbMGLRq1cqoIwcBJi0iImWRYUaM6OhoAED79u119q9cuRKDBw8GAMyfPx8WFhbo168f8vLy0KlTJ3z55ZeGxVkCJi0iIiXRAJCMcA09iDIkORsbGyxevBiLFy9+yqDKhve0iIhIMdjSIiJSEGOOHlQiJi0iIiXhLO9ERETKwJYWEZGSmHlLi0mLiEhJmLSIiEgxZBjyXpHwnhYRESkGW1pERArCIe9ERKQcZn5Pi92DRESkGGxpEREpiUYAkoEtJY1yW1pMWkRESmLm3YOKTlpFMw/f1+SbvmyNPGNGJRmKvV+Qa/pCAdwXpv+9AoCmME+WcoUMf8cAABn+lkVhocnLBABNrun/lovKLMtM6fRkik5ad+7cAQDEp6+SN5Bn3VW5AzCx23IHQOXmQ/mKvnPnDtRqtRGuZISWlr5LF1cgik5anp6eSElJgYODAyRJv6ftsrOz4eXlhZSUFDg6OpZThBWHOdXXnOoKmFd9lVhXIQTu3LkDT09PY12Q3YNKZWFhgerVqxt0DUdHR8X88RuDOdXXnOoKmFd9lVZX47SwCFB40iIiMjsaAYO79zh6kIiITEJoHmyGXkOhzPbhYpVKhWnTpkGlUskdikmYU33Nqa6AedXXnOpKJZMEx2ESEVV42dnZUKvVCPR6B1YWhiXt+5o8/JYSjaysLEXdGwTYPUhEpCy8p0VERIph5kPezfaeFhERKQ9bWkRESiJghJaWUSKRBVtapDiDBw9G7969tT+3b98e4eHhJo8jPj4ekiQhMzOz1GMkSUJMTEyZrzl9+nQ0btzYoLguX74MSZJw9OhRg65DFVRR96Chm0IxaZFRDB48GJIkQZIkWFtbo1atWoiMjMT9+/fLvezvvvsOM2fOLNOxZUk0RFRxsXuQjKZz585YuXIl8vLy8NNPPyEsLAyVKlXClClTih2bn58Pa2tro5Tr7OxslOsQKYJGA8DAh4NlWqXCGNjSIqNRqVRwd3eHt7c33nnnHQQGBuKHH34A8G+X3qxZs+Dp6YnatWsDAFJSUjBgwAA4OTnB2dkZvXr1wuXLl7XXLCwsxIQJE+Dk5AQXFxe8//77xZZ4eLR7MC8vD5MmTYKXlxdUKhVq1aqFr7/+GpcvX0aHDh0AAFWqVIEkSRg8eDAAQKPRICoqCr6+vrC1tUWjRo2wZcsWnXJ++uknvPDCC7C1tUWHDh104iyrSZMm4YUXXkDlypVRs2ZNREREoKCgoNhxS5cuhZeXFypXrowBAwYgKytL5/WvvvoKderUgY2NDfz9/fHll1/qHQspFLsHicqHra0t8vP/XSMqLi4OycnJiI2NxY4dO1BQUIBOnTrBwcEBv//+O/744w/Y29ujc+fO2vPmzp2LVatWYcWKFdi3bx8yMjKwbdu2x5b71ltvYcOGDVi4cCFOnz6NpUuXwt7eHl5eXti6dSsAIDk5GTdu3MDnn38OAIiKisKaNWuwZMkSnDx5EuPHj8egQYOwZ88eAA+Sa9++fdGjRw8cPXoUw4YNw+TJk/V+TxwcHLBq1SqcOnUKn3/+OZYvX4758+frHHP+/Hls2rQJ27dvx86dO/Hnn39i1KhR2tfXrVuHqVOnYtasWTh9+jRmz56NiIgIrF69Wu94iJSG3YNkdEIIxMXF4ZdffsGYMWO0++3s7PDVV19puwW/+eYbaDQafPXVV9qlZVauXAknJyfEx8cjKCgICxYswJQpU9C3b18AwJIlS/DLL7+UWvbZs2exadMmxMbGIjAwEABQs2ZN7etFXYmurq5wcnIC8KBlNnv2bPz2229o1aqV9px9+/Zh6dKlaNeuHaKjo+Hn54e5c+cCAGrXro3jx49jzpw5er03H37474JOPj4+mDhxIjZu3Ij3339fuz83Nxdr1qzBc889BwBYtGgRunXrhrlz58Ld3R3Tpk3D3Llzte+Jr68vTp06haVLlyIkJESveEiBzPw5LSYtMpodO3bA3t4eBQUF0Gg0eOONNzB9+nTt6w0aNNC5j3Xs2DGcP38eDg4OOtfJzc3FhQsXkJWVhRs3bqBly5ba16ysrNCsWbNSV4E9evQoLC0t0a5duzLHff78edy7dw8dO3bU2Z+fn48mTZoAAE6fPq0TBwBtgtPHt99+i4ULF+LChQu4e/cu7t+/X2wanRo1amgTVlE5Go0GycnJcHBwwIULFzB06FAMHz5ce8z9+/e5/IW54IwYRMbRoUMHREdHw9raGp6enrCy0v3zsrOz0/n57t27aNq0KdatW1fsWtWqVXuqGGxtbfU+5+7duwCAH3/8USdZADDqxKwJCQkIDg7GjBkz0KlTJ6jVamzcuFHbetMn1uXLlxdLopaWlkaLlaiiYtIio7Gzs0OtWrXKfPyLL76Ib7/9Fq6urqVO2unh4YGDBw+ibdu2AB60KBITE/Hiiy+WeHyDBg2g0WiwZ88ebffgw4paeoWFhdp9devWhUqlwtWrV0ttodWpU0c7qKTIgQMHnlzJh+zfvx/e3t744IMPtPuuXLlS7LirV6/i+vXr2pVuDxw4AAsLC9SuXRtubm7w9PTExYsXERwcrFf59GwQQgNh4NIihp4vJw7EINkEBwejatWq6NWrF37//XdcunQJ8fHxGDt2LP766y8AwLhx4/Dxxx8jJiYGZ86cwahRox77jJWPjw9CQkIwZMgQxMTEaK+5adMmAIC3tzckScKOHTtw8+ZN3L17Fw4ODpg4cSLGjx+P1atX48KFC0hKSsKiRYu0gxvefvttnDt3Du+99x6Sk5Oxfv16rFq1Sq/6Pv/887h69So2btyICxcuYOHChSUOKrGxsUFISAiOHTuG33//HWPHjsWAAQPg7u4OAJgxYwaioqKwcOFCnD17FsePH8fKlSsxb948veIhhRLiQfeeIZuC72kxaZFsKleujL1796JGjRro27cv6tSpg6FDhyI3N1fb8nr33Xfx5ptvIiQkBK1atYKDgwP69Onz2OtGR0ejf//+GDVqFPz9/TF8+HDk5OQAAJ577jnMmDEDkydPhpubG0aPHg0AmDlzJiIiIhAVFYU6deqgc+fO+PHHH+Hr6wvgwX2mrVu3IiYmBo0aNcKSJUswe/Zsverbs2dPjB8/HqNHj0bjxo2xf/9+REREFDuuVq1a6Nu3L7p27YqgoCA0bNhQZ0j7sGHD8NVXX2HlypVo0KAB2rVrh1WrVmljJXqWcT0tIiIFKFpP6xX1m7CSDHsw/77IR1zWWq6nRURE5UyjASQD70kp+J4WkxYRkZIIIwx5V3AHG+9pERGRYrClRUSkIEKjgTCwe1DJQ96ZtIiIlITdg0RERMrAlhYRkZJoBCCZb0uLSYuISEmEgMGLQCo4abF7kIiIFIMtLSIiBREaAWFg96CSJ0JiS4uISEmExjjbU1i8eDF8fHxgY2ODli1b4tChQ0au3JMxaRER0RN9++23mDBhAqZNm4akpCQ0atQInTp1Qnp6uknjYNIiIlIQoRFG2fQ1b948DB8+HKGhoahbty6WLFmCypUrY8WKFeVQy9IxaRERKYkM3YP5+flITEzUWVjVwsICgYGBSEhIMHYNH4sDMYiIFOQ+CgyeEOM+CgA8WO7kYSqVCiqVqtjxt27dQmFhIdzc3HT2u7m54cyZM4YFoycmLSIiBbC2toa7uzv2pf5klOvZ29vDy8tLZ9+0adMwffp0o1y/vDBpEREpgI2NDS5duoT8/HyjXE8IAUmSdPaV1MoCgKpVq8LS0hJpaWk6+9PS0uDu7m6UeMqKSYuISCFsbGxgY2Nj8nKtra3RtGlTxMXFoXfv3gAAjUaDuLg4jB492qSxMGkREdETTZgwASEhIWjWrBlatGiBBQsWICcnB6GhoSaNg0mLiIie6LXXXsPNmzcxdepUpKamonHjxti5c2exwRnlTRJKns+DiIjMCp/TIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixWDSIiIixfh/Ze5iNYZY7j4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score for ClassificationTree on cross validation is:  0.5192277932528628\n"
     ]
    }
   ],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "# your code here\n",
    "\n",
    "#multiclass classification using classification tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def cross_validation_multiclass(model, features, responses, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "    confusion_matrix = np.zeros((10,10))\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        model.train(features[train_index], responses[train_index])\n",
    "        preds = np.array([model.predict(features[t]) for t in test_index])\n",
    "        #calculate accuracy from preds and responses for classification\n",
    "        scores.append(np.sum((preds == responses[test_index]))/len(responses[test_index]))\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            confusion_matrix[responses[test_index][i]][preds[i]] += 1\n",
    "\n",
    "    #plot confusion matrix\n",
    "    plt.matshow(confusion_matrix)\n",
    "    #plot the title \n",
    "    plt.title('Confusion matrix for Multiclass Classification')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # print(scores)\n",
    "    return np.mean(scores)\n",
    "\n",
    "cross_val_score = cross_validation_multiclass(ClassificationTree(classes=np.unique(digits.target)), digits.data, digits.target)\n",
    "print(\"The mean accuracy score for ClassificationTree on cross validation is: \",cross_val_score)\n",
    "\n",
    "\n",
    "#multiclass classification using classification forest\n",
    "\n",
    "# cross_val_score = cross_validation_multiclass(ClassificationForest(n_trees=10, classes=np.unique(digits.target)), digits.data, digits.target)\n",
    "# print(\"The mean accuracy score for ClassificationForest on cross validation is: \",cross_val_score)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Result:\n",
    "As seen above, the multiclass classifier with ClassificationTree performs poorly than binary classifier with same model, this is because there are 10 classes vs 2 in previous case, secondly the multiclass model has to predict 10 classes vs 2 in previous case so the possibly of predicting exactly the correct class wrong is much higher. This can be correct by using soft response instead of hard response, where the model predicts the predicted class based on posterior probability of each class.\n",
    "\n",
    "For multiclass classificationForest model, it is an ensemble of 10 multi-class classification trees, so the performance is better than single multi-class classification tree. This is to reduce overfitting and increase the accuracy of the model. The accuracy of the model can be further improved by using soft voting instead of hard voting of single trees used in classificationForest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> solultion has values we only plot </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RegressionForest.__init__() missing 1 required positional argument: 'n_trees'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m-n 1 -r 1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mimport seaborn\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# Train ten one-against-the-rest regression forests for the 10 digits.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# Make sure that all training sets are balanced between the current digit and the rest.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# Assign test instances to the digit with highest score, \u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# or to \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39munknown\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m if all scores are negative.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfeatures = digits.data\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mlabels = digits.target\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39msplitter = KFold(n_splits=5)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mclasses = list(np.unique(labels))\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mclasses.append(\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39munknown\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mforest = RegressionForest()\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mconfusion_matrix = np.zeros((len(classes), len(classes)))\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfor train_index, test_index in splitter.split(features):\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    forest.train(features[train_index], labels[train_index])\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m    for sample in test_index:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        label = labels[sample]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        prediction = forest.predict(features[sample])\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m        confusion_matrix[classes.index(label)][classes.index(prediction)] += 1\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mprint(\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mAccuracy: \u001b[39;49m\u001b[39m{:.1%}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.format(np.trace(confusion_matrix) / np.sum(confusion_matrix)))\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mplt.figure(figsize=(10, 8))\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mseaborn.heatmap(confusion_matrix / np.sum(confusion_matrix, axis=1), annot=True, cmap=\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mBlues\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m, xticklabels=classes, yticklabels=classes)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mplt.xlabel(\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mPredicted label\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mplt.ylabel(\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mTrue label\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mplt.title(\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39mConfusion matrix after 5-fold cross-validation\u001b[39;49m\u001b[39m\\'\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mplt.show()\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2477\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2478\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2480\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/execution.py:1174\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[1;32m   1172\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m all_runs \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mrepeat(repeat, number)\n\u001b[1;32m   1175\u001b[0m best \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(all_runs) \u001b[39m/\u001b[39m number\n\u001b[1;32m   1176\u001b[0m worst \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(all_runs) \u001b[39m/\u001b[39m number\n",
      "File \u001b[0;32m/usr/lib/python3.10/timeit.py:206\u001b[0m, in \u001b[0;36mTimer.repeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    204\u001b[0m r \u001b[39m=\u001b[39m []\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(repeat):\n\u001b[0;32m--> 206\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[1;32m    207\u001b[0m     r\u001b[39m.\u001b[39mappend(t)\n\u001b[1;32m    208\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/magics/execution.py:158\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    156\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    159\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:15\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: RegressionForest.__init__() missing 1 required positional argument: 'n_trees'"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "import seaborn\n",
    "\n",
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "features = digits.data\n",
    "labels = digits.target\n",
    "\n",
    "splitter = KFold(n_splits=5)\n",
    "classes = list(np.unique(labels))\n",
    "classes.append(\"unknown\")\n",
    "\n",
    "forest = RegressionForest()\n",
    "confusion_matrix = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "for train_index, test_index in splitter.split(features):\n",
    "    forest.train(features[train_index], labels[train_index])\n",
    "\n",
    "    for sample in test_index:\n",
    "        label = labels[sample]\n",
    "        prediction = forest.predict(features[sample])\n",
    "        confusion_matrix[classes.index(label)][classes.index(prediction)] += 1\n",
    "\n",
    "print(\"Accuracy: {:.1%}\".format(np.trace(confusion_matrix) / np.sum(confusion_matrix)))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "seaborn.heatmap(confusion_matrix / np.sum(confusion_matrix, axis=1), annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion matrix after 5-fold cross-validation')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green; font-weight:\n",
    "bold\"> produces error doesn't work </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
