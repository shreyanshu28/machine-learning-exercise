{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from abc import abstractmethod\n",
    "from math import log, e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "        \n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index\n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min # your code here\n",
    "        \n",
    "\n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        return np.random.choice(D, size=D_try, replace=False) # your code here\n",
    "        \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        # sort the feature values along feature j\n",
    "        sorted_features = np.sort(node.features[:, j])\n",
    "\n",
    "        # find the candidate thresholds (in the middle between adjacent feature values)\n",
    "        thresholds = (sorted_features[:-1] + sorted_features[1:]) / 2\n",
    "\n",
    "        return thresholds # your code here\n",
    "       \n",
    "               \n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        # your code here\n",
    "        # split the data according to the threshold t\n",
    "        left_mask = node.features[:, j] <= t\n",
    "        right_mask = node.features[:, j] > t\n",
    "\n",
    "        # assign the instances to the children\n",
    "        left.features = node.features[left_mask]\n",
    "        left.responses = node.responses[left_mask]\n",
    "        right.features = node.features[right_mask]\n",
    "        right.responses = node.responses[right_mask]\n",
    "\n",
    "        return left, right \n",
    "        \n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"compute_loss_for_split() must be implemented in a subclass.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=10):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        \n",
    "        # your code here\n",
    "        #compute loss\n",
    "        left_mask = node.features[:, j] <= t\n",
    "        right_mask = node.features[:, j] > t\n",
    "\n",
    "        # check if the split is admissible\n",
    "        if np.sum(left_mask) < self.n_min or np.sum(right_mask) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        # compute the loss for the split\n",
    "        left_responses = node.responses[left_mask]\n",
    "        right_responses = node.responses[right_mask]\n",
    "\n",
    "        # squared loss for regression\n",
    "        left_loss = np.mean((left_responses - np.mean(left_responses)) ** 2)\n",
    "        right_loss = np.mean((right_responses - np.mean(right_responses)) ** 2)\n",
    "        loss = left_loss + right_loss\n",
    "\n",
    "        return loss\n",
    "       \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        node.prediction =  np.mean(node.responses) # your code here\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        self.criterion = 'entropy' # or 'gini'\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        # your code here\n",
    "        left_mask = node.features[:, j] <= t\n",
    "        right_mask = node.features[:, j] > t\n",
    "\n",
    "        # check if the split is admissible\n",
    "        if np.sum(left_mask) < self.n_min or np.sum(right_mask) < self.n_min:\n",
    "            return float('inf')\n",
    "\n",
    "        # compute the loss for the split\n",
    "        left_responses = node.responses[left_mask]\n",
    "        right_responses = node.responses[right_mask]\n",
    "\n",
    "        left_p = np.mean(left_responses == 1)\n",
    "        right_p = np.mean(right_responses == 1)\n",
    "        left_loss = self.entropy(left_p) if self.criterion == 'entropy' else self.gini_impurity(left_p)\n",
    "        right_loss = self.entropy(right_p) if self.criterion == 'entropy' else self.gini_impurity(right_p)\n",
    "        loss = (np.sum(left_mask) * left_loss + np.sum(right_mask) * right_loss) / node.features.shape[0]\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def entropy(self, p):\n",
    "        ''' compute the entropy\n",
    "        '''\n",
    "        value,counts = np.unique(p, return_counts=True)\n",
    "        norm_counts = counts / counts.sum()\n",
    "        base = e\n",
    "        return -(norm_counts * np.log(norm_counts)/np.log(base)).sum()\n",
    "    \n",
    "    def gini_impurity(self, p):\n",
    "        ''' compute the Gini impurity\n",
    "        '''\n",
    "        return 1 - p ** 2 - (1 - p) ** 2\n",
    "        \n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        # your code here\n",
    "        #considering the case where there are more than 2 classes class labels as well, the minlengh of bin\n",
    "        counts_bins = np.bincount(node.responses, minlength=max(self.classes) + 1)\n",
    "        # print(counts_bins)\n",
    "        pred_counts = counts_bins[self.classes]  #only select indices that are in self.classes (others are zero)\n",
    "        # print(np.argmax(pred_counts), pred_counts)\n",
    "        node.prediction = self.classes[np.argmax(pred_counts)]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9)\n",
    "features = digits.data[instances, :]\n",
    "labels = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses = np.array([1 if l == 3 else -1 for l in labels])\n",
    "\n",
    "assert(features.shape[0] == labels.shape[0] == responses.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean MSE score for RegressionTree on cross validation is:  0.405244662688346\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "# your code here\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation(model, features, responses, n_splits=5, regression = False):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        model.train(features[train_index], responses[train_index])\n",
    "        preds = np.array([model.predict(features[t]) for t in test_index])\n",
    "        if regression:\n",
    "            #for regression we calculate MSE\n",
    "            scores.append(np.sum((preds - responses[test_index]) ** 2) / len(responses[test_index]))\n",
    "        else:\n",
    "            #calculate accuracy from preds and responses for classification\n",
    "            scores.append(np.sum((preds == responses[test_index]))/len(responses[test_index]))\n",
    "    # print(scores)\n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"The mean MSE score for RegressionTree on cross validation is: \", cross_validation(RegressionTree(n_min=5), features, responses, regression=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score for ClassificationTree on cross validation is:  0.7023972602739725\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# your code here\n",
    "# print(np.unique(labels))\n",
    "print(\"The mean accuracy score for ClassificationTree on cross validation is: \",cross_validation(ClassificationTree(classes=np.unique(labels)), features, labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "    ... # your code here\n",
    "    raise NotImplementedError(\"bootstrap_sampling(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        ... # your code here\n",
    "        raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        ... # your code here\n",
    "        raise NotImplementedError(\"predict(): remove this exception after adding your code above.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "# your code here\n",
    "\n",
    "cross_val_score = cross_validation(RegressionForest(n_trees=10, n_min=5), features, responses, regression=True)\n",
    "print(\"The mean MSE score for RegressionForest on cross validation is: \", cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# your code here\n",
    "\n",
    "cross_val_score = cross_validation(ClassificationForest(n_trees=10, classes=np.unique(labels)), features, labels)\n",
    "\n",
    "print(\"The mean accuracy score for ClassificationTree on cross validation is: \",cross_val_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAAD+CAYAAAADOAHeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjE0lEQVR4nO3dfbxcVX3v8c83JyEPEAIkASEJJArCRapAU0TwUhRbEbiAvag8aFGxSKuAQq+K7atQq77U+gC1ahsBhYIIAhaslkeliBUkICIEgZTHQCBPQAJEknPO9/6x1sDk5Jwz+8zZZ/bM5Pd+veZ1Zvbes/Zv9sz+nbXXXntt2SaEEFptXNUBhBA2TZF8QgiViOQTQqhEJJ8QQiUi+YQQKhHJJ4RQiUqTj6TJkn4k6TlJPxhFOcdJuq7M2Koi6X9Lur/J9+4q6S5JaySdUnZszZL0XUmfHWb+pyWdO9pyWk3STZI+NEZl7yjpeUk9+fV2km7O3+1Xim6zJtbbsn2pUPKRdKykhXljLJX0n5LeXML6jwK2A6bbflezhdi+2PaflhDPmJJkSTsPt4ztn9vetclVfAL4me2ptv+pyTJeJumsHPOpA6afmqef1USZB0paUj/N9udtj8lOPBqSNsvb4EFJL0h6RNL5kuaO9bptP2Z7C9t9edKJwApgS9unl7HNJM3N3+P4uvW2bF9qmHwknQacDXyelCh2BL4JHFHC+ncCHrDdW0JZHa/+R9CknYB7S173A8CfD5h2fJ7e7S4HDgeOBaYBbwDuAA6qIJadgEXupl7Btod8kDb488C7hllmIik5PZkfZwMT87wDgSXA6cAyYCnwgTzv74F1wPq8jhOAs4CL6sqeCxgYn1+/H3gIWAM8DBxXN/2WuvftB9wOPJf/7lc37ybgH4Bf5HKuA2YM8dlq8X+iLv4jgUNIO98q4NN1y+8D/BJ4Ni/7z8Bmed7N+bO8kD/ve+rK/yTwFPBvtWn5Pa/J69g7v94BWA4cOEisPwX6gN/n8l+bv78L83seBf4WGFe3zX4BfA1YCXx2kDLPAi4C7gNel6e9DliUp5812PbP0wzsnJ9/F/gssDmwFujPMT6fP9PA7/3NwH/n7fg48P76cvLzrYH/yJ/tmfx8dl0Z72fw38rOwH+RfhsrgEuH+O7flmOdM8xv/ybgQ3Xf1U/ztlwBXAxsVbfsJ4Encjz3AwfV/WYWAquBp4GvDvzt58+9nrS/PJ9jK7rNDgV+nct/vPad5XmP5XXUvos3DfwuKWlfGnT7NUg+BwO95J1/iGU+A9wKbAvMzBvgH+p23t68zATSTvsisHX9j3vgj32w5EP64a4Gds3ztueVHeLlDQZsQ/oxvi+/75j8enrdBvsf0s45Ob/+wjDJpxf4uxz/X5B+7N8DppJ2xLXAvLz8HwL75vXOJe20HxtshxxQ/hdJSXwydcknL/MXpJ19CnAt8OUiO0N+fSFwVY51LilhnlC3zXqBk3O8k4dJPp8GvpinfQk4gyaST31CH2w9+flOpB/yMXmbTwf2HKSc6cD/zdtlKvAD4N/zvOF+K5cAf0Oq9U8C3jzEtvwC8F8N9o+Xtzcpqf1J/h5nkv7ZnJ3n7Ura8Xeo+12/Jj//JfC+/HwLYN+Bv/2Bn32E2+xA4A/y5309KcEdOdg6xnJfGuzR6LBrOrDCwx8WHQd8xvYy28tJNZr31c1fn+evt/0TUoZttk2jH9hD0mTbS20PdohxKPCg7X+z3Wv7EuB3wP+pW+Y7th+wvRa4DNhzmHWuBz5nez3wfWAGcI7tNXn9i0jVcWzfYfvWvN5HgH8F/rjAZzrT9ks5ng3Y/jawGLiNtBP9TYPyAMgNlUcDZ+RYHwG+wobfzZO2v57j3WjddS4CjpE0IZd5UZEYmnQscIPtS/JvZqXtuwYulKdfYftF22uAz7Hhth7qt7KetLPuYPv3tm8ZIo7ppNprIbYX274+f4/Lga/WxdNHSkq7S5pg+xHb/1MXz86SZth+3vatRddZZ8htZvsm27+13W/7blLybfSbrCl7X9pAo+SzEpjRoC1iB1KVvubRPO3lMgYkrxdJGX5EbL9AOlQ5CVgq6ceSdisQTy2mWXWvnxpBPCv9SqNfbQd9um7+2tr7Jb1W0n9IekrSalI72YxhygZYbvv3DZb5NrAH8HXbLzVYtmYG6b/gwO+mfjs8XqQg24+REuDnST/GQu9r0hzSf9NhSZoi6V8lPZq39c3AVpJ6GvxWPgEI+JWkeyV9cIhVrCQl+0Ly2ajvS3oix3MR+bu3vRj4GKm2siwvV9tHTiDVHH4n6XZJhxVdZ50ht5mkN0r6maTlkp4jbZNGv8masvelDTRKPr8EXiK1cwzlSdJ/kpod87RmvECqRte8qn6m7Wtt/wnpR/E70k7ZKJ5aTE80GdNIfIsU1y62tyQdrqjBe4ZtQJS0Bakd7TzgLEnbFIxlBa/8l68ZuB1G0nh5Iant7sJB5m3wvUl61SDLFF3n46T2k0ZOJ9Wg35i39QG11cPQvxXbT9n+C9s7AB8GvjnEGcgbgH0kzS4QC6TEbOAPcjzvpe67t/09228mfR8mHWpj+0Hbx5CaLb4IXC5p84LrrBlum30PuJrUdjUN+Je6uBp9F2O6Lw2bfGw/R2rv+IakI/N/mwmS3iHpS3mxS4C/lTRT0oy8fLPV8ruAA3Ifh2mktgXg5f8sR+Qv5iXS4Vv/IGX8BHht7h4wXtJ7gN1JDZJjbSqpreH5/J/2LwfMfxp49QjLPAdY6HRa9cekH09DubZ2GfA5SVMl7QScRvPfzaXAn+YyB/oN8DpJe0qaRPoPP5Sngen5+x3MxcDbJL07f3/TJe05yHJTSbXOZ3NCPrM2Y7jfiqR31SWUZ0g74Ea/I9s3ANcDP5T0hzmWqZJOGqK2NDWv5zlJs4D/VxfPrpLeKmki6YRArdEdSe+VNNN2P6mxmMHiaWC4bTYVWGX795L2IR2i1SzP6xrqNzmm+1LDU+22v0L60f5tDvZx4KPAv+dFPktqrb8b+C1wZ542YravJ/3I7yad0qz/kONyHE+SzgD9MRvv3NheCRxG+s+4klTNPsz2imZiGqG/Jn25a0j/aS8dMP8s4AJJz0p6d6PCJB1BavSvfc7TgL0lHVcwnpNJtZKHgFtI/wXPL/jeDdhea/uGIdqlHiCdVLgBeDCva6hyfkf6h/VQ3g47DJj/GOnExOmk7/kucpvaAGeTGjlXkE54XFM3b7jfyh8Bt0l6nlQjONX2Q0OEexRpB7yUdLbnHmB+/pwD/T2wd17ux8CVdfMmkhqwV5AOU7bllX+sBwP35njOAY5u0P62kQbb7K+Az0haQ6oYXFb3vhdJbWW/yN/FvgPKHdN9SbnVOoQQWmq0ndpCCG3i7W+Z4hWrih2x3Xn3S9faPniMQxpWJJ8QusSKVX389zWzGi8ITNrh4aJnvMZMJJ8QukRqOe+cZpRIPiF0kf4RnyirTiSfELqEMX0ddAJpkxxMTNLBku6XtFjSpyqMY07ufboo97Y9tapY6knqkfRrSa3oG9Uolq0kXS7pd5Luk/SmiuP5eP6u7pF0Se7X1Db6caFHO9jkkk++5ukbwDtIHaaOkbR7ReH0Aqfb3p10QepHKoyl3qmki2LbwTnANbZ3I/VdqSyu3HnwFGC+7T2A2vVzbcFAHy70aAebXPIhDWGw2PZDtteRLhY9oopA8gWPd+bna0g7VrHTFWMk9/49FCh9lLwmYplGumziPADb62w/W2lQeQSAfL3jFJq/lKh0Bta7v9CjHWyKyWcWG15QuYSKd3hIo8oBe5GuXq/S2aSerO3wC51H6lX/nXwYeG4T1z2VxvYTwJdJ4+AsBZ6z3VbD9/YXfLSDTTH5tJ188egVpLF/VlcYx2HAMtt3VBXDAONJlyx8y/ZepEtFqmyj25pUS55HuuJ7c0nvrSqegVzwkCsOu6rzBGkIgprZtOaK90EpjZFzBXCx7SsbLT/G9gcOl/QI6XD0rZLGcuyeRpaQBh6r1QYvJyWjqrwNeNj28jy+05Wkkf7ag6Gv4KMdbIrJ53ZgF0nzJG1GajC8uopAJInUnnGf7a9WEUM922fYnm17Lmm7/NR2Zf/ZbT8FPC6pNvjcQaTB26ryGLBvHt1BOZ52aZh/+fL8Tjns2uT6+djulfRR0pCkPcD5HnxExFbYnzSy4G8l3ZWnfdppxMeQnAxcnP9RPAR8oKpAbN8m6XLSyA29pLGRF1QVz8ZEX8Pho9pHXNUeQpfY4/Wb+YofF7tka7cdl95he/4YhzSsTa7mE0K3MrCug1pSIvmE0EX63TmHXZF8QugSqYdzJJ8QQosZ0ReHXSGEKnTSYVfnpMmSSTqx6hjqRTxDa6dYoP3iqakddhV5tINNNvkA7fYDiniG1k6xQPvFk4k+jyv0aAdx2BVClzCwnp6qwyisK5PPjG16PHfOhGGX2XHWeOa/YdKwPSwfuHvKcLOLK1DLncQUthy3zfA9PkvqD6oJjb/2ST1bMG2zbYdeY3/rOqdOGrcF08bPHH6FJR1JuLev4TKTmMKWGv670rhyaher+1eusD2zyLK22qZWU0RXJp+5cybwq2vnNF6wgbfvsOfogwE0vpzN7N7exgsVMH7mcHczLsZrR3Rfu6GppJ2lp5xy+lauKqWccVsUvmX5sK5b/Z2B90ofVn+btOcU0ZXJJ4RNUWpwjppPCKHlOuuwq3MiDSEMKw2pMa7QoxFJ50taJumeumn/mAfyv1vSDyVtVTfvjHxDhvslvb1IvB2TfNrljhMhtLM+q9CjgO8CA2+nfD2wh+3XAw8AZwDkmx4cDbwuv+eb+UYNw+qI5NNmd5wIoS0Zsd7jCz0almXfDKwaMO0627WzHreSRgGFNLTs922/ZPthYDHpRg3D6ojkQxvdcSKEdlVrcC7yKMEHgf/Mz5u6KUOnNDgP9uHeWFEsIbQlU/iQCmCGpIV1rxfYLjQqo6S/IY3kePEIQ9xApySfhvL1NidC6kAYwqaoSGNytqKZkQwlvR84DDjIrwyD2tRNGTrlsKvhh7O9wPZ82/NnTu+cLuYhlMVmTK/tknQw6Z5uh9t+sW7W1cDRkiZKmgfsAvyqUXmdUkV4+Y4TpKRzNHBstSGF0G5UWg9nSZcAB5IOz5YAZ5LObk0Erk837+BW2yfZvlfSZaQ7i/QCH7Hd8DqVjkg+bXbHiRDakoF1Bc5kFSrLPmaQyecNs/zngM+NZB0dkXwA8u1k4pYyIQzBqKMGE+uY5BNCaCyu7QohtJyB/g66tiuSTwhdo32GSC2iK5PPA3dP4e2z9hp1OS9cM6+EaGDaSeWMw+NnnyulHCZPGn0ZJY3no0kTSynHW5Qz8Nv4LTYvpRxeWldOOauLLxo1nxBCZaLmE0JoOVus7++cXbpzIg0hDCuN5xM1nxBCy3XWSIaRfELoEqnBOWo+IYQKRCfDEELLxeUVIYTKjGA8n8pF8gmhS9iwvj+STwihxdJhVySfEEIFoodzCKHl4lR7CKEicdgVQqhIXF4RQmi5dPeKSD4hhBYzore/c24bFcknhC4Sh10V07hxjJs8edTlTPvw+hKigb2ueqSUcu58z66llNN7/+LRF6JyfuRl/Z/WZhNKKqkcfaueafk642xXCKEynXS2q3MiDSEMz+nC0iKPRiSdL2mZpHvqpm0j6XpJD+a/W+fpkvRPkhZLulvS3kXCjeQTQpeojWRY5FHAd4GDB0z7FHCj7V2AG/NrgHeQ7s++C3Ai8K0iK4jkE0IXKavmY/tmYNWAyUcAF+TnFwBH1k2/0MmtwFaStm+0jmjzCaFLGOgtflX7DEkL614vsL2gwXu2s700P38K2C4/nwU8XrfckjxtKcPoiOQjaQ5wIenDmrShzqk2qhDaywgHE1the37T67Ityc2+Hzok+QC9wOm275Q0FbhD0vW2F1UdWAjtZIz7+TwtaXvbS/Nh1bI8/QlgTt1ys/O0YXVEm4/tpbbvzM/XAPeRqnUhhBqX1+YzhKuB4/Pz44Gr6qb/eT7rtS/wXN3h2ZA6pebzMklzgb2A2yoOJYS2UmYnQ0mXAAeS2oaWAGcCXwAuk3QC8Cjw7rz4T4BDgMXAi8AHiqyjo5KPpC2AK4CP2V49YN6JpNN8TFJJ99sOocOUlXxsHzPErIMGWdbAR0a6jo5JPpImkBLPxbavHDg/t9QvAJjWM2NUDWEhdCIj+mIM53JJEnAecJ/tr1YdTwjtqpMuLO2UNLk/8D7grZLuyo9Dqg4qhHbisW9wLlVH1Hxs3wIdlNJDqIjbJLEU0RHJJ4RQRPvUaoqI5BNCF4maTwih5WIwsTZg9+N160ZfzgsvlhAN/PK0fUopZ9WXXiilnG2PGH0ZPTvPG30hQN+DD5VSTs8O2zVeqIhlK0spZtyOJXXAf2AEy8YA8iGEKpg47AohVCIanEMIFXEH9e2P5BNCF4nDrhBCy9mRfEIIFYk2nxBCJfr7I/mEEFrMKA67QgjV6KCTXZF8Quga0eAcQqhMB1V9IvmE0EWi5hNCqET0cB5A0tcZpkJo+5RWxBFCN7PBMYD8RhY2XiSEMFpR8xnA9gX1ryVNsV3OYDkhhFeUmHwkfRz4UC71t6SbAW4PfB+YDtwBvM92U4NntbSOJulNkhYBv8uv3yDpm62MIYTulToZFnk0LEmaBZwCzLe9B9ADHA18Efia7Z2BZ4ATmo221Q3OZwNvJ93bGdu/kXRA2SvRhAn07LD9qMvx6udLiAYm3v1IKeVse0Q5o+ydvvjeUZfx1ddNLCES6Nl151LK4cmnSynG69aXUk7/ylWllDNi5R52jQcmS1oPTAGWAm8Fjs3zLwDOAr7VTOEtb52y/fiASX2tjiGErpQ7GRas+cyQtLDuceIGRdlPAF8GHiMlnedIh1nP2u7Niy0Bmh4vttU1n8cl7Qc43/74VOC+FscQQvcqXvNZYXv+UDMlbQ0cAcwDngV+ABw8yug20Oqaz0mkG8rPAp4E9qSJG8yHEIZgFXs09jbgYdvLba8HriTdOXgrSbVKy2zgiWZDbWnNx/YK4LhWrjOETUp5bT6PAftKmgKsBQ4idZn5GXAU6YzX8cBVza6g1We7Xi3pR5KWS1om6SpJr25lDCF0LVNazcf2bcDlwJ2k0+zjgAXAJ4HTJC0mnW4/r9lwW93m8z3gG8A78+ujgUuANzZ6o6QeUuZ9wvZhYxZhCB2szE6Gts8Ezhww+SGglBvRtbrNZ4rtf7Pdmx8XAZMKvjcap0NoxAUfbaAlyUfSNpK2Af5T0qckzZW0k6RPAD8p8P7ZwKHAuWMdawgdrbwG5zHXqsOuO0j5tvapP1w3z8AZDd5/NvAJYOpQC+R+CicCTOoZcrEQupdB/VUHUVyrru1q+sbekg4Dltm+Q9KBw6xjAalBjGkTt2uTimUIrdQ+tZoiWj6ej6Q9gN2pa+uxfeEwb9kfOFzSIfk9W0q6yPZ7xzbSEDpQB/3bbfWp9jOBr+fHW4AvAYcP9x7bZ9iebXsu6ezYTyPxhDCEaHAe0lGkzkpP2f4A8AZgWotjCKF7dVDyafVh11rb/ZJ6JW0JLAPmFH2z7ZuAm8YothA6W62TYYdodfJZKGkr4NukM2DPA79scQwhdC21Sa2miFZf2/VX+em/SLoG2NL23a2MIYSuFslnQ5L2Hm6e7TtLXiH09Iy+nO1njr4MQM+sLqWc8bObHjplA1/ba/T9oBZ/5zUlRAKvPWXg8E7NcV85HVw0Z4dSyhn/UlMji27s4ZEtHjWfjX1lmHkmjY4WQhitaPPZkO23tGI9IWzS2uhMVhFx08AQukkknxBCFaLNJ4RQjQ5KPq2+vEKS3ivp7/LrHSWVMjBRCJs65avaizzaQasvr/gm8CbgmPx6DWlkwxBCGWI8nyG90fbekn4NYPsZSZu1OIYQulcHHXa1Ovmsz2MxG0DSTKBNKoEhdL5OanBu9WHXPwE/BLaV9DngFuDzLY4hhO4VV7UPzvbFku4gDash4EjbMSh8CGVwZ9V8Wpp8JO0IvAj8qH6a7cdaGUcIXSuSz5B+zCsDyU8i3Qf6fuB1LY4jhK5U5mn0PPzNucAepP32g6T99VJgLvAI8G7bzzRTfkvbfGz/ge3X57+7kG4+FuP5hNCezgGusb0badTR+4BPATfm/ffG/LoprW5w3kAeSqPh3UpDCAWV1OAsaRpwAPl2yLbX2X4WOAK4IC92AXBks6G2us3ntLqX44C9gSdbGUMIXWtkDc4zJC2se70g336qZh6wHPiOpDeQRh49FdjO9tK8zFPAds2G2+o2n/pRrHpJbUBXtDiGELpX8eSzwvb8YeaPJ1UOTrZ9m6RzGHCIZdtS8+fXWpZ8cufCqbb/esxX1tePX3hx9OU889zoywD6e3vLKWfNmlLK6ZkxfdRlvPavRjjE3hB+eO8NpZRz5G7ljEfX/0hJIyuuK2kkwxGvuLSSlgBLbN+WX19OSj5PS9re9lJJ25NuAtGUVt2rfbztPtINAEMIY0Dki0sLPBqx/RTwuKRd86SDgEXA1cDxedrxwFXNxtuqms+vSFW4uyRdDfwAeKE20/aVLYojhO5V/r3aTwYuztdfPgR8gFRhuUzSCcCjwLubLbzVbT6TgJWkMZtr/X0MRPIJoQwldjK0fRcwWLvQQWWU36rks20+03UPrySdmg7qkxlCm+ugvalVyacH2IINk05NB22uENpbXNu1saW2PzOaAgbr6m07ekeHUC+Sz0bKGDqt1tX7qNwANqWEMkPoHm00XEYRrUo+o2qgquvq/X5IXb2BijpShNC+2mV85iJa0s/H9qpRFlHf1fvXks6VtHkJoYXQVcrq59MKlV5YOgK1rt7fsr0XqY/QBl29JZ0oaaGkhev611YRYwjV66CRDDsl+QzW1Xvv+gVsL7A93/b8zcZNbnmAIVSuaOKJ5FPcMF29QwiZRvBoB510x9LBunqHEOq1Sa2miI5JPsN09Q4hZO3SmFxExySfEEIBHXSqPZJPCN2ijU6jFxHJJ4RuEsmnWu7ro//Z0Y9C2POqbUuIBihjVEWA58s5T6EtpzZeqAGvaupuKRs5fNYflVLOOxc9VEo5PzrkD0spp39lOduH1SNbPGo+IYRqRPIJIVQhaj4hhNZro97LRUTyCaFLiM66qj2STwjdJGo+IYQqyJ2TfSL5hNAtos0nhFCVTjrb1RFDaoQQCip5PB9JPXn00P/Ir+dJuk3SYkmX5lEmmhLJJ4QuMgbDqJ4K3Ff3+ovA12zvDDwDnNBsrJF8QugW+XbJRR5FSJoNHEq6ZRWSRLrb8OV5kQuAI5sNN5JPCN2k3MOus4FP8MpAHdOBZ2335tdLgFnNhhrJJ4QuIUZ02DWjdsOF/Dhxg7Kkw4Bltu8Yq3jjbFcI3aR4P58VtocbGXR/4HBJhwCTgC1JN+7cStL4XPuZDTzRbKhR8wmhi5TV4Gz7DNuzbc8FjgZ+avs44GfAUXmx44Grmo01kk8I3aI1t875JHCapMWkNqDzmi0oDrtC6CJjcWGp7ZuAm/Lzh4B9yii3S5OPoa9v1KX0bbvV6EMB3LN1KeVo3g6llOPFS0ZfyMzpoy8DGNdXzt5y9RGvLqWcJX/2qlLKmXPJulLKGfFIhnFVewih5cxIGpwrF8knhC7SSdd2RfIJoZtE8gkhtFqtk2GniOQTQrewO6rNp2P6+Uj6uKR7Jd0j6RJJk6qOKYR2U+aFpWOtI5KPpFnAKcB823sAPaRelyGEOmMwpMaY6aTDrvHAZEnrgSnAkxXHE0J7MdDfJpmlgI6o+dh+Avgy8BiwFHjO9nXVRhVCGxr7yytK0xHJR9LWwBHAPGAHYHNJ7x2wzIm14QHW+6Uqwgyhcp102NURyQd4G/Cw7eW21wNXAvvVL2B7ge35tudP0MRKggyhcrUzXo0ebaBT2nweA/aVNAVYCxwELKw2pBDaT7vUaoroiORj+zZJlwN3Ar3Ar4EF1UYVQnuRQR3U4NwRyQfA9pnAmVXHEUJba5M+PEV0TPIJITQWt0sOIbReG51GLyKSTwhdo33OZBXRlclH4yfQ86rtRl2OV68tIRrg6eWlFPP7+TuXUs7kSaPvitD/VDmfyWvL2cZavrKUcuZcsb6Ucu7/SjkjInLMyBaPs10hhGpEzSeE0HIG9UXyCSFUoXNyTySfELpJJ51q75Rru0IIRZR0bZekOZJ+JmlRHsTv1Dx9G0nXS3ow/236vlCRfELoFib1cC7yaKwXON327sC+wEck7Q58CrjR9i7Ajfl1UyL5hNAlhJGLPRqxvdT2nfn5GuA+YBZpaJsL8mIXAEc2G2+0+YTQTcagzUfSXGAv4DZgO9tL86yngKY71EXyCaFbGCh+qn2GpPphaRbY3mikCElbAFcAH7O9WtIrq7MtNd+tMZJPCF1kBGe7VtieP2xZ0gRS4rnY9pV58tOStre9VNL2wLJmY402nxC6SXlnuwScB9xn+6t1s64Gjs/PjweuajbUqPmE0DVKvbB0f+B9wG8l3ZWnfRr4AnCZpBOAR4F3N7uCSD4hdAtTWvKxfQvpDsyDOaiMdUTyCaGbxEiGIYQqdNLlFZF8QugWBvo6p+oTySeErhEjGVZu9fplK655/JxHGyw2A1jRingKahzP9a0JJGun7dM4lmdKWlOxchrHM8IRCIex04iWjuRTLdszGy0jaWGjTlatFPEMrZ1igfaLZwORfEIILWcgbhoYQmg9g6PBuRO02+2WI56htVMs0H7xJHG2qzMMdgVvlSKeobVTLNB+8Wwg2nxCCJWI5BNCaL3O6ucTQ2p0CEl9ku6SdI+kH0iaMoqyvivpqPz83Dw271DLHihpvybW8YikGUWnD1jm+RGu6yxJfz3SGLuOgf7+Yo82EMmnc6y1vaftPYB1wEn1MyU1VYu1/SHbi4ZZ5EBgxMknVKSk8XxaIZJPZ/o5sHOulfxc0tXAIkk9kv5R0u2S7pb0YUgDQ0n6Z0n3S7oB2LZWkKSbJM3Pzw+WdKek30i6MY/dexLw8Vzr+t+SZkq6Iq/jdkn75/dOl3Rdvs3KuQw9HMPLJP27pDvye04cMO9refqNkmbmaa+RdE1+z88l7VbK1uwmHZR8os2nw+QazjuAa/KkvYE9bD+cd+DnbP+RpInALyRdRxr8e1dgd9KA34uA8weUOxP4NnBALmsb26sk/QvwvO0v5+W+B3zN9i2SdgSuBf4XcCZwi+3PSDoUOKHAx/lgXsdk4HZJV9heCWwOLLT9cUl/l8v+KOkU90m2H5T0RuCbwFub2IzdycZ9fVVHUVgkn84xuW5EuZ+ThrjcD/iV7Yfz9D8FXl9rzwGmAbsABwCX2O4DnpT000HK3xe4uVaW7VVDxPE2YPe6gcS3zIOMHwD8WX7vjyUVuUrqFEnvzM/n5FhXkkaluTRPvwi4Mq9jP+AHdeueWGAdm5bo4RzGwFrbe9ZPyDvhC/WTgJNtXztguUNKjGMcsK/t3w8SS2GSDiQlsjfZflHSTcCkIRZ3Xu+zA7dBGKBNDqmKiDaf7nIt8Jf5rgNIeq2kzYGbgffkNqHtgbcM8t5bgQMkzcvv3SZPXwNMrVvuOuDk2gtJe+anNwPH5mnvABrdRnca8ExOPLuRal4144Ba7e1Y0uHcauBhSe/K65CkNzRYx6bFjrNdoTLnktpz7pR0D/CvpNrtD4EH87wLgV8OfKPt5cCJpEOc3/DKYc+PgHfWGpyBU4D5uUF7Ea+cdft7UvK6l3T49ViDWK8Bxku6jzQo+a11814A9smf4a3AZ/L044ATcnz3ku6eGep1UIOz3CaBhBBGZ1rPDO87+dBCy173woV3VD0sSLT5hNA12qdWU0QknxC6hYEOOtUebT4hdAkD7nehRxG50+n9khZL+lTZ8UbyCaFbOA8mVuTRgKQe4BukDq27A8cMdw1gMyL5hNBFSqz57AMstv2Q7XXA9yn57GIknxC6SUk1H2AW8Hjd6yV5WmmiwTmELrGGZ669wZcPO1xJnUmSFta9XtDqERoj+YTQJWwfXGJxT5Cut6uZnaeVJg67QgiDuR3YRdI8SZsBRwNXl7mCqPmEEDZiu1fSR0nXC/YA59u+t8x1xOUVIYRKxGFXCKESkXxCCJWI5BNCqEQknxBCJSL5hBAqEcknhFCJSD4hhEpE8gkhVOL/A6FbkGHDIpPrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score for ClassificationTree on cross validation is:  0.5737712782420303\n"
     ]
    }
   ],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "# your code here\n",
    "\n",
    "#multiclass classification using classification tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def cross_validation_multiclass(model, features, responses, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    scores = []\n",
    "    confusion_matrix = np.zeros((10,10))\n",
    "    for train_index, test_index in kf.split(features):\n",
    "        model.train(features[train_index], responses[train_index])\n",
    "        preds = np.array([model.predict(features[t]) for t in test_index])\n",
    "        #calculate accuracy from preds and responses for classification\n",
    "        scores.append(np.sum((preds == responses[test_index]))/len(responses[test_index]))\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            confusion_matrix[responses[test_index][i]][preds[i]] += 1\n",
    "\n",
    "    #plot confusion matrix\n",
    "    plt.matshow(confusion_matrix)\n",
    "    #plot the title \n",
    "    plt.title('Confusion matrix for Multiclass Classification')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # print(scores)\n",
    "    return np.mean(scores)\n",
    "\n",
    "cross_val_score = cross_validation_multiclass(ClassificationTree(classes=np.unique(digits.target)), digits.data, digits.target)\n",
    "print(\"The mean accuracy score for ClassificationTree on cross validation is: \",cross_val_score)\n",
    "\n",
    "\n",
    "#multiclass classification using classification forest\n",
    "\n",
    "# cross_val_score = cross_validation_multiclass(ClassificationForest(n_trees=10, classes=np.unique(digits.target)), digits.data, digits.target)\n",
    "# print(\"The mean accuracy score for ClassificationForest on cross validation is: \",cross_val_score)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on Result:\n",
    "As seen above, the multiclass classifier with ClassificationTree performs poorly than binary classifier with same model, this is because there are 10 classes vs 2 in previous case, secondly the multiclass model has to predict 10 classes vs 2 in previous case so the possibly of predicting exactly the correct class wrong is much higher. This can be correct by using soft response instead of hard response, where the model predicts the predicted class based on posterior probability of each class.\n",
    "\n",
    "For multiclass classificationForest model, it is an ensemble of 10 multi-class classification trees, so the performance is better than single multi-class classification tree. This is to reduce overfitting and increase the accuracy of the model. The accuracy of the model can be further improved by using soft voting instead of hard voting of single trees used in classificationForest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "... # your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
